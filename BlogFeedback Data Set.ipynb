{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysing the 'BlogFeedback Data Set' from the UC Irvine Machine Learning repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is used to analyze the 'BlogFeedback Data Set' from the UC Irvine Machine Learning repository. The data set is available [here](https://archive.ics.uci.edu/ml/datasets/BlogFeedback). **The objective of the notebook is to create a model to predict the number of comments in a blog post in the upcoming 24 hours**.\n",
    "\n",
    "This data originates from blog posts. The raw HTML-documents of the blog posts were crawled and processed. In the train data, the basetimes were in the years 2010 and 2011. In the test data the basetimes were in February and March 2012.\n",
    "\n",
    "**The data set has 280 attributes. Therefore, in this notebooks we test different techniques to deal with this large number of attributes**. First, we analyze the whole data set without any kind of adjustment. This will be our reference model. Then, we test some feature selection methods to identify the most relevant attributes to predict the target value. Finally, we test the Principal Component Analysis (PCA) dimensionality reduction method.\n",
    "\n",
    "The notebook is organized as follows:\n",
    "\n",
    "1. Data exploration\n",
    "2. Train ML model\n",
    "3. Evaluate the ML model\n",
    "4. Conclusion\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we explore the characteristics of the data set, including its dimensions and characteristics of its variables.\n",
    "\n",
    "The data set contains 281 columns and 52397 rows.\n",
    "\n",
    "The attributes of the data set are the following:\n",
    "\n",
    "Column:\n",
    "- 1...50: Average, standard deviation, min, max and median of the Attributes 51...60 for the source of the current blog post. With source we mean the blog on which the post appeared. For example, myblog.blog.org would be the source of the post myblog.blog.org/post_2010_09_10\n",
    "- 51: Total number of comments before basetime\n",
    "- 52: Number of comments in the last 24 hours before the basetime\n",
    "- 53: Let T1 denote the datetime 48 hours before basetime. Let T2 denote the datetime 24 hours before basetime. This attribute is the number of comments in the time period between T1 and T2\n",
    "- 54: Number of comments in the first 24 hours after the publication of the blog post, but before basetime\n",
    "- 55: The difference of Attribute 52 and Attribute 53\n",
    "- 56...60: The same features as the attributes 51...55, but features 56...60 refer to the number of links (trackbacks), while features 51...55 refer to the number of comments.\n",
    "- 61: The length of time between the publication of the blog post and basetime\n",
    "- 62: The length of the blog post\n",
    "- 63...262: The 200 bag of words features for 200 frequent words of the text of the blog post\n",
    "- 263...269: binary indicator features (0 or 1) for the weekday (Monday...Sunday) of the basetime\n",
    "- 270...276: binary indicator features (0 or 1) for the weekday (Monday...Sunday) of the date of publication of the blog post\n",
    "- 277: Number of parent pages: we consider a blog post P as a parent of blog post B, if B is a reply (trackback) to blog post P.\n",
    "- 278...280: Minimum, maximum, average number of comments that the parents received\n",
    "- 281: The target: the number of comments in the next 24 hours (relative to basetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 52397 entries, 0 to 52396\n",
      "Columns: 281 entries, 1 to 281\n",
      "dtypes: float64(281)\n",
      "memory usage: 112.3 MB\n"
     ]
    }
   ],
   "source": [
    "attributes = [*range(1, 282, 1)]\n",
    "\n",
    "df_data = pd.read_csv('/Users/leuzinger/Dropbox/Data Science/Awari/Regressions/BlogFeedback/blogData_train.csv',names=attributes)\n",
    "df_data.reset_index(inplace=False)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "df_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "att=[]\n",
    "for i in ['total','last24h','24-48h','first24h','difference',\n",
    "           'total_tr','last24h_tr','24-48h_tr','first24h_tr','difference_tr']:\n",
    "    att1 = 'blog_avg_' + str(i)\n",
    "    att2 = 'blog_std_' + str(i)\n",
    "    att3 = 'blog_min_' + str(i)\n",
    "    att4 = 'blog_max_' + str(i)\n",
    "    att5 = 'blog_median_' + str(i)\n",
    "    att.extend([att1,att2,att3,att4,att5])\n",
    "\n",
    "att51_62 = ['total','last24h','24-48h','first24h','difference',\n",
    "           'total_tr','last24h_tr','24-48h_tr','first24h_tr','difference_tr',\n",
    "           'time_first_post','lenght_post']\n",
    "att.extend(att51_62)\n",
    "\n",
    "for i in range(63,263):\n",
    "    att_word = 'word' + str(i-62)\n",
    "    att.extend([att_word])\n",
    "\n",
    "att263_281 = ['Mon_bl','Tue_bl','Wed_bl','Thu_bl','Fri_bl','Sat_bl','Sun_bl',\n",
    "             'Mon_post','Tue_post','Wed_post','Thu_post','Fri_post','Sat_post','Sun_post',\n",
    "             'parent_pages','min_parent','max_parent','avg_parent','target']\n",
    "att.extend(att263_281)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blog_avg_total</th>\n",
       "      <th>blog_std_total</th>\n",
       "      <th>blog_min_total</th>\n",
       "      <th>blog_max_total</th>\n",
       "      <th>blog_median_total</th>\n",
       "      <th>blog_avg_last24h</th>\n",
       "      <th>blog_std_last24h</th>\n",
       "      <th>blog_min_last24h</th>\n",
       "      <th>blog_max_last24h</th>\n",
       "      <th>blog_median_last24h</th>\n",
       "      <th>blog_avg_24-48h</th>\n",
       "      <th>blog_std_24-48h</th>\n",
       "      <th>blog_min_24-48h</th>\n",
       "      <th>blog_max_24-48h</th>\n",
       "      <th>blog_median_24-48h</th>\n",
       "      <th>blog_avg_first24h</th>\n",
       "      <th>blog_std_first24h</th>\n",
       "      <th>blog_min_first24h</th>\n",
       "      <th>blog_max_first24h</th>\n",
       "      <th>blog_median_first24h</th>\n",
       "      <th>blog_avg_difference</th>\n",
       "      <th>blog_std_difference</th>\n",
       "      <th>blog_min_difference</th>\n",
       "      <th>blog_max_difference</th>\n",
       "      <th>blog_median_difference</th>\n",
       "      <th>blog_avg_total_tr</th>\n",
       "      <th>blog_std_total_tr</th>\n",
       "      <th>blog_min_total_tr</th>\n",
       "      <th>blog_max_total_tr</th>\n",
       "      <th>blog_median_total_tr</th>\n",
       "      <th>blog_avg_last24h_tr</th>\n",
       "      <th>blog_std_last24h_tr</th>\n",
       "      <th>blog_min_last24h_tr</th>\n",
       "      <th>blog_max_last24h_tr</th>\n",
       "      <th>blog_median_last24h_tr</th>\n",
       "      <th>blog_avg_24-48h_tr</th>\n",
       "      <th>blog_std_24-48h_tr</th>\n",
       "      <th>blog_min_24-48h_tr</th>\n",
       "      <th>blog_max_24-48h_tr</th>\n",
       "      <th>blog_median_24-48h_tr</th>\n",
       "      <th>blog_avg_first24h_tr</th>\n",
       "      <th>blog_std_first24h_tr</th>\n",
       "      <th>blog_min_first24h_tr</th>\n",
       "      <th>blog_max_first24h_tr</th>\n",
       "      <th>blog_median_first24h_tr</th>\n",
       "      <th>blog_avg_difference_tr</th>\n",
       "      <th>blog_std_difference_tr</th>\n",
       "      <th>blog_min_difference_tr</th>\n",
       "      <th>blog_max_difference_tr</th>\n",
       "      <th>blog_median_difference_tr</th>\n",
       "      <th>total</th>\n",
       "      <th>last24h</th>\n",
       "      <th>24-48h</th>\n",
       "      <th>first24h</th>\n",
       "      <th>difference</th>\n",
       "      <th>total_tr</th>\n",
       "      <th>last24h_tr</th>\n",
       "      <th>24-48h_tr</th>\n",
       "      <th>first24h_tr</th>\n",
       "      <th>difference_tr</th>\n",
       "      <th>time_first_post</th>\n",
       "      <th>lenght_post</th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>word5</th>\n",
       "      <th>word6</th>\n",
       "      <th>word7</th>\n",
       "      <th>word8</th>\n",
       "      <th>word9</th>\n",
       "      <th>word10</th>\n",
       "      <th>word11</th>\n",
       "      <th>word12</th>\n",
       "      <th>word13</th>\n",
       "      <th>word14</th>\n",
       "      <th>word15</th>\n",
       "      <th>word16</th>\n",
       "      <th>word17</th>\n",
       "      <th>word18</th>\n",
       "      <th>word19</th>\n",
       "      <th>word20</th>\n",
       "      <th>word21</th>\n",
       "      <th>word22</th>\n",
       "      <th>word23</th>\n",
       "      <th>word24</th>\n",
       "      <th>word25</th>\n",
       "      <th>word26</th>\n",
       "      <th>word27</th>\n",
       "      <th>word28</th>\n",
       "      <th>word29</th>\n",
       "      <th>word30</th>\n",
       "      <th>word31</th>\n",
       "      <th>word32</th>\n",
       "      <th>word33</th>\n",
       "      <th>word34</th>\n",
       "      <th>word35</th>\n",
       "      <th>word36</th>\n",
       "      <th>word37</th>\n",
       "      <th>word38</th>\n",
       "      <th>word39</th>\n",
       "      <th>word40</th>\n",
       "      <th>word41</th>\n",
       "      <th>word42</th>\n",
       "      <th>word43</th>\n",
       "      <th>word44</th>\n",
       "      <th>word45</th>\n",
       "      <th>word46</th>\n",
       "      <th>word47</th>\n",
       "      <th>word48</th>\n",
       "      <th>word49</th>\n",
       "      <th>word50</th>\n",
       "      <th>word51</th>\n",
       "      <th>word52</th>\n",
       "      <th>word53</th>\n",
       "      <th>word54</th>\n",
       "      <th>word55</th>\n",
       "      <th>word56</th>\n",
       "      <th>word57</th>\n",
       "      <th>word58</th>\n",
       "      <th>word59</th>\n",
       "      <th>word60</th>\n",
       "      <th>word61</th>\n",
       "      <th>word62</th>\n",
       "      <th>word63</th>\n",
       "      <th>word64</th>\n",
       "      <th>word65</th>\n",
       "      <th>word66</th>\n",
       "      <th>word67</th>\n",
       "      <th>word68</th>\n",
       "      <th>word69</th>\n",
       "      <th>word70</th>\n",
       "      <th>word71</th>\n",
       "      <th>word72</th>\n",
       "      <th>word73</th>\n",
       "      <th>word74</th>\n",
       "      <th>word75</th>\n",
       "      <th>word76</th>\n",
       "      <th>word77</th>\n",
       "      <th>word78</th>\n",
       "      <th>word79</th>\n",
       "      <th>word80</th>\n",
       "      <th>word81</th>\n",
       "      <th>word82</th>\n",
       "      <th>word83</th>\n",
       "      <th>word84</th>\n",
       "      <th>word85</th>\n",
       "      <th>word86</th>\n",
       "      <th>word87</th>\n",
       "      <th>word88</th>\n",
       "      <th>word89</th>\n",
       "      <th>word90</th>\n",
       "      <th>word91</th>\n",
       "      <th>word92</th>\n",
       "      <th>word93</th>\n",
       "      <th>word94</th>\n",
       "      <th>word95</th>\n",
       "      <th>word96</th>\n",
       "      <th>word97</th>\n",
       "      <th>word98</th>\n",
       "      <th>word99</th>\n",
       "      <th>word100</th>\n",
       "      <th>word101</th>\n",
       "      <th>word102</th>\n",
       "      <th>word103</th>\n",
       "      <th>word104</th>\n",
       "      <th>word105</th>\n",
       "      <th>word106</th>\n",
       "      <th>word107</th>\n",
       "      <th>word108</th>\n",
       "      <th>word109</th>\n",
       "      <th>word110</th>\n",
       "      <th>word111</th>\n",
       "      <th>word112</th>\n",
       "      <th>word113</th>\n",
       "      <th>word114</th>\n",
       "      <th>word115</th>\n",
       "      <th>word116</th>\n",
       "      <th>word117</th>\n",
       "      <th>word118</th>\n",
       "      <th>word119</th>\n",
       "      <th>word120</th>\n",
       "      <th>word121</th>\n",
       "      <th>word122</th>\n",
       "      <th>word123</th>\n",
       "      <th>word124</th>\n",
       "      <th>word125</th>\n",
       "      <th>word126</th>\n",
       "      <th>word127</th>\n",
       "      <th>word128</th>\n",
       "      <th>word129</th>\n",
       "      <th>word130</th>\n",
       "      <th>word131</th>\n",
       "      <th>word132</th>\n",
       "      <th>word133</th>\n",
       "      <th>word134</th>\n",
       "      <th>word135</th>\n",
       "      <th>word136</th>\n",
       "      <th>word137</th>\n",
       "      <th>word138</th>\n",
       "      <th>word139</th>\n",
       "      <th>word140</th>\n",
       "      <th>word141</th>\n",
       "      <th>word142</th>\n",
       "      <th>word143</th>\n",
       "      <th>word144</th>\n",
       "      <th>word145</th>\n",
       "      <th>word146</th>\n",
       "      <th>word147</th>\n",
       "      <th>word148</th>\n",
       "      <th>word149</th>\n",
       "      <th>word150</th>\n",
       "      <th>word151</th>\n",
       "      <th>word152</th>\n",
       "      <th>word153</th>\n",
       "      <th>word154</th>\n",
       "      <th>word155</th>\n",
       "      <th>word156</th>\n",
       "      <th>word157</th>\n",
       "      <th>word158</th>\n",
       "      <th>word159</th>\n",
       "      <th>word160</th>\n",
       "      <th>word161</th>\n",
       "      <th>word162</th>\n",
       "      <th>word163</th>\n",
       "      <th>word164</th>\n",
       "      <th>word165</th>\n",
       "      <th>word166</th>\n",
       "      <th>word167</th>\n",
       "      <th>word168</th>\n",
       "      <th>word169</th>\n",
       "      <th>word170</th>\n",
       "      <th>word171</th>\n",
       "      <th>word172</th>\n",
       "      <th>word173</th>\n",
       "      <th>word174</th>\n",
       "      <th>word175</th>\n",
       "      <th>word176</th>\n",
       "      <th>word177</th>\n",
       "      <th>word178</th>\n",
       "      <th>word179</th>\n",
       "      <th>word180</th>\n",
       "      <th>word181</th>\n",
       "      <th>word182</th>\n",
       "      <th>word183</th>\n",
       "      <th>word184</th>\n",
       "      <th>word185</th>\n",
       "      <th>word186</th>\n",
       "      <th>word187</th>\n",
       "      <th>word188</th>\n",
       "      <th>word189</th>\n",
       "      <th>word190</th>\n",
       "      <th>word191</th>\n",
       "      <th>word192</th>\n",
       "      <th>word193</th>\n",
       "      <th>word194</th>\n",
       "      <th>word195</th>\n",
       "      <th>word196</th>\n",
       "      <th>word197</th>\n",
       "      <th>word198</th>\n",
       "      <th>word199</th>\n",
       "      <th>word200</th>\n",
       "      <th>Mon_bl</th>\n",
       "      <th>Tue_bl</th>\n",
       "      <th>Wed_bl</th>\n",
       "      <th>Thu_bl</th>\n",
       "      <th>Fri_bl</th>\n",
       "      <th>Sat_bl</th>\n",
       "      <th>Sun_bl</th>\n",
       "      <th>Mon_post</th>\n",
       "      <th>Tue_post</th>\n",
       "      <th>Wed_post</th>\n",
       "      <th>Thu_post</th>\n",
       "      <th>Fri_post</th>\n",
       "      <th>Sat_post</th>\n",
       "      <th>Sun_post</th>\n",
       "      <th>parent_pages</th>\n",
       "      <th>min_parent</th>\n",
       "      <th>max_parent</th>\n",
       "      <th>avg_parent</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.567566</td>\n",
       "      <td>48.475178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.479934</td>\n",
       "      <td>46.18691</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.076167</td>\n",
       "      <td>1.795416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400491</td>\n",
       "      <td>1.078097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377559</td>\n",
       "      <td>1.07421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>1.704671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022932</td>\n",
       "      <td>1.521174</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.567566</td>\n",
       "      <td>48.475178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.479934</td>\n",
       "      <td>46.18691</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.076167</td>\n",
       "      <td>1.795416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400491</td>\n",
       "      <td>1.078097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377559</td>\n",
       "      <td>1.07421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>1.704671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022932</td>\n",
       "      <td>1.521174</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.567566</td>\n",
       "      <td>48.475178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.479934</td>\n",
       "      <td>46.18691</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.076167</td>\n",
       "      <td>1.795416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400491</td>\n",
       "      <td>1.078097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377559</td>\n",
       "      <td>1.07421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>1.704671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022932</td>\n",
       "      <td>1.521174</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.567566</td>\n",
       "      <td>48.475178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.479934</td>\n",
       "      <td>46.18691</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.076167</td>\n",
       "      <td>1.795416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400491</td>\n",
       "      <td>1.078097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377559</td>\n",
       "      <td>1.07421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>1.704671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022932</td>\n",
       "      <td>1.521174</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40.30467</td>\n",
       "      <td>53.845657</td>\n",
       "      <td>0.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>15.52416</td>\n",
       "      <td>32.44188</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14.044226</td>\n",
       "      <td>32.615417</td>\n",
       "      <td>0.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>34.567566</td>\n",
       "      <td>48.475178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>378.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.479934</td>\n",
       "      <td>46.18691</td>\n",
       "      <td>-356.0</td>\n",
       "      <td>377.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.076167</td>\n",
       "      <td>1.795416</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400491</td>\n",
       "      <td>1.078097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377559</td>\n",
       "      <td>1.07421</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972973</td>\n",
       "      <td>1.704671</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.022932</td>\n",
       "      <td>1.521174</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blog_avg_total  blog_std_total  blog_min_total  blog_max_total  \\\n",
       "0        40.30467       53.845657             0.0           401.0   \n",
       "1        40.30467       53.845657             0.0           401.0   \n",
       "2        40.30467       53.845657             0.0           401.0   \n",
       "3        40.30467       53.845657             0.0           401.0   \n",
       "4        40.30467       53.845657             0.0           401.0   \n",
       "\n",
       "   blog_median_total  blog_avg_last24h  blog_std_last24h  blog_min_last24h  \\\n",
       "0               15.0          15.52416          32.44188               0.0   \n",
       "1               15.0          15.52416          32.44188               0.0   \n",
       "2               15.0          15.52416          32.44188               0.0   \n",
       "3               15.0          15.52416          32.44188               0.0   \n",
       "4               15.0          15.52416          32.44188               0.0   \n",
       "\n",
       "   blog_max_last24h  blog_median_last24h  blog_avg_24-48h  blog_std_24-48h  \\\n",
       "0             377.0                  3.0        14.044226        32.615417   \n",
       "1             377.0                  3.0        14.044226        32.615417   \n",
       "2             377.0                  3.0        14.044226        32.615417   \n",
       "3             377.0                  3.0        14.044226        32.615417   \n",
       "4             377.0                  3.0        14.044226        32.615417   \n",
       "\n",
       "   blog_min_24-48h  blog_max_24-48h  blog_median_24-48h  blog_avg_first24h  \\\n",
       "0              0.0            377.0                 2.0          34.567566   \n",
       "1              0.0            377.0                 2.0          34.567566   \n",
       "2              0.0            377.0                 2.0          34.567566   \n",
       "3              0.0            377.0                 2.0          34.567566   \n",
       "4              0.0            377.0                 2.0          34.567566   \n",
       "\n",
       "   blog_std_first24h  blog_min_first24h  blog_max_first24h  \\\n",
       "0          48.475178                0.0              378.0   \n",
       "1          48.475178                0.0              378.0   \n",
       "2          48.475178                0.0              378.0   \n",
       "3          48.475178                0.0              378.0   \n",
       "4          48.475178                0.0              378.0   \n",
       "\n",
       "   blog_median_first24h  blog_avg_difference  blog_std_difference  \\\n",
       "0                  12.0             1.479934             46.18691   \n",
       "1                  12.0             1.479934             46.18691   \n",
       "2                  12.0             1.479934             46.18691   \n",
       "3                  12.0             1.479934             46.18691   \n",
       "4                  12.0             1.479934             46.18691   \n",
       "\n",
       "   blog_min_difference  blog_max_difference  blog_median_difference  \\\n",
       "0               -356.0                377.0                     0.0   \n",
       "1               -356.0                377.0                     0.0   \n",
       "2               -356.0                377.0                     0.0   \n",
       "3               -356.0                377.0                     0.0   \n",
       "4               -356.0                377.0                     0.0   \n",
       "\n",
       "   blog_avg_total_tr  blog_std_total_tr  blog_min_total_tr  blog_max_total_tr  \\\n",
       "0           1.076167           1.795416                0.0               11.0   \n",
       "1           1.076167           1.795416                0.0               11.0   \n",
       "2           1.076167           1.795416                0.0               11.0   \n",
       "3           1.076167           1.795416                0.0               11.0   \n",
       "4           1.076167           1.795416                0.0               11.0   \n",
       "\n",
       "   blog_median_total_tr  blog_avg_last24h_tr  blog_std_last24h_tr  \\\n",
       "0                   0.0             0.400491             1.078097   \n",
       "1                   0.0             0.400491             1.078097   \n",
       "2                   0.0             0.400491             1.078097   \n",
       "3                   0.0             0.400491             1.078097   \n",
       "4                   0.0             0.400491             1.078097   \n",
       "\n",
       "   blog_min_last24h_tr  blog_max_last24h_tr  blog_median_last24h_tr  \\\n",
       "0                  0.0                  9.0                     0.0   \n",
       "1                  0.0                  9.0                     0.0   \n",
       "2                  0.0                  9.0                     0.0   \n",
       "3                  0.0                  9.0                     0.0   \n",
       "4                  0.0                  9.0                     0.0   \n",
       "\n",
       "   blog_avg_24-48h_tr  blog_std_24-48h_tr  blog_min_24-48h_tr  \\\n",
       "0            0.377559             1.07421                 0.0   \n",
       "1            0.377559             1.07421                 0.0   \n",
       "2            0.377559             1.07421                 0.0   \n",
       "3            0.377559             1.07421                 0.0   \n",
       "4            0.377559             1.07421                 0.0   \n",
       "\n",
       "   blog_max_24-48h_tr  blog_median_24-48h_tr  blog_avg_first24h_tr  \\\n",
       "0                 9.0                    0.0              0.972973   \n",
       "1                 9.0                    0.0              0.972973   \n",
       "2                 9.0                    0.0              0.972973   \n",
       "3                 9.0                    0.0              0.972973   \n",
       "4                 9.0                    0.0              0.972973   \n",
       "\n",
       "   blog_std_first24h_tr  blog_min_first24h_tr  blog_max_first24h_tr  \\\n",
       "0              1.704671                   0.0                  10.0   \n",
       "1              1.704671                   0.0                  10.0   \n",
       "2              1.704671                   0.0                  10.0   \n",
       "3              1.704671                   0.0                  10.0   \n",
       "4              1.704671                   0.0                  10.0   \n",
       "\n",
       "   blog_median_first24h_tr  blog_avg_difference_tr  blog_std_difference_tr  \\\n",
       "0                      0.0                0.022932                1.521174   \n",
       "1                      0.0                0.022932                1.521174   \n",
       "2                      0.0                0.022932                1.521174   \n",
       "3                      0.0                0.022932                1.521174   \n",
       "4                      0.0                0.022932                1.521174   \n",
       "\n",
       "   blog_min_difference_tr  blog_max_difference_tr  blog_median_difference_tr  \\\n",
       "0                    -8.0                     9.0                        0.0   \n",
       "1                    -8.0                     9.0                        0.0   \n",
       "2                    -8.0                     9.0                        0.0   \n",
       "3                    -8.0                     9.0                        0.0   \n",
       "4                    -8.0                     9.0                        0.0   \n",
       "\n",
       "   total  last24h  24-48h  first24h  difference  total_tr  last24h_tr  \\\n",
       "0    2.0      2.0     0.0       2.0         2.0       0.0         0.0   \n",
       "1    6.0      2.0     4.0       5.0        -2.0       0.0         0.0   \n",
       "2    6.0      2.0     4.0       5.0        -2.0       0.0         0.0   \n",
       "3    2.0      2.0     0.0       2.0         2.0       0.0         0.0   \n",
       "4    3.0      1.0     2.0       2.0        -1.0       0.0         0.0   \n",
       "\n",
       "   24-48h_tr  first24h_tr  difference_tr  time_first_post  lenght_post  word1  \\\n",
       "0        0.0          0.0            0.0             10.0          0.0    0.0   \n",
       "1        0.0          0.0            0.0             35.0          0.0    0.0   \n",
       "2        0.0          0.0            0.0             35.0          0.0    0.0   \n",
       "3        0.0          0.0            0.0             10.0          0.0    0.0   \n",
       "4        0.0          0.0            0.0             34.0          0.0    0.0   \n",
       "\n",
       "   word2  word3  word4  word5  word6  word7  word8  word9  word10  word11  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0     0.0   \n",
       "1    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0     0.0   \n",
       "2    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0     0.0   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0     0.0   \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0     0.0   \n",
       "\n",
       "   word12  word13  word14  word15  word16  word17  word18  word19  word20  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word21  word22  word23  word24  word25  word26  word27  word28  word29  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word30  word31  word32  word33  word34  word35  word36  word37  word38  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word39  word40  word41  word42  word43  word44  word45  word46  word47  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word48  word49  word50  word51  word52  word53  word54  word55  word56  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word57  word58  word59  word60  word61  word62  word63  word64  word65  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word66  word67  word68  word69  word70  word71  word72  word73  word74  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word75  word76  word77  word78  word79  word80  word81  word82  word83  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word84  word85  word86  word87  word88  word89  word90  word91  word92  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word93  word94  word95  word96  word97  word98  word99  word100  word101  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0      0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0      0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0      0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0      0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0      0.0   \n",
       "\n",
       "   word102  word103  word104  word105  word106  word107  word108  word109  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word110  word111  word112  word113  word114  word115  word116  word117  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word118  word119  word120  word121  word122  word123  word124  word125  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word126  word127  word128  word129  word130  word131  word132  word133  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word134  word135  word136  word137  word138  word139  word140  word141  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word142  word143  word144  word145  word146  word147  word148  word149  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word150  word151  word152  word153  word154  word155  word156  word157  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word158  word159  word160  word161  word162  word163  word164  word165  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word166  word167  word168  word169  word170  word171  word172  word173  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word174  word175  word176  word177  word178  word179  word180  word181  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word182  word183  word184  word185  word186  word187  word188  word189  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word190  word191  word192  word193  word194  word195  word196  word197  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word198  word199  word200  Mon_bl  Tue_bl  Wed_bl  Thu_bl  Fri_bl  Sat_bl  \\\n",
       "0      0.0      0.0      0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "1      0.0      0.0      0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "2      0.0      0.0      0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "3      0.0      0.0      0.0     0.0     0.0     0.0     0.0     1.0     0.0   \n",
       "4      0.0      0.0      0.0     0.0     0.0     0.0     0.0     0.0     1.0   \n",
       "\n",
       "   Sun_bl  Mon_post  Tue_post  Wed_post  Thu_post  Fri_post  Sat_post  \\\n",
       "0     0.0       0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "1     0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "2     0.0       0.0       0.0       1.0       0.0       0.0       0.0   \n",
       "3     0.0       0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "4     0.0       0.0       0.0       0.0       1.0       0.0       0.0   \n",
       "\n",
       "   Sun_post  parent_pages  min_parent  max_parent  avg_parent  target  \n",
       "0       0.0           0.0         0.0         0.0         0.0     1.0  \n",
       "1       0.0           0.0         0.0         0.0         0.0     0.0  \n",
       "2       0.0           0.0         0.0         0.0         0.0     0.0  \n",
       "3       0.0           0.0         0.0         0.0         0.0     1.0  \n",
       "4       0.0           0.0         0.0         0.0         0.0    27.0  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.set_axis(att,axis=1,inplace=True)\n",
    "df_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our analysis looking to which attributes have the higher correlation with the price. First, we create a correlation matrix. \n",
    "\n",
    "We can see that the varibales that have the stronger postive correlations with the target value are the blog_median_last24h and blog_avg_difference. Besides, we see that the total blog publications, publications in the last 24h and publications between 24h-48h have a strong correlation with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target                       1.000000\n",
       "blog_median_last24h          0.506540\n",
       "blog_avg_difference          0.503375\n",
       "blog_avg_last24h             0.497631\n",
       "blog_median_total            0.491707\n",
       "blog_avg_24-48h              0.490111\n",
       "blog_median_24-48h           0.489674\n",
       "blog_median_first24h         0.486316\n",
       "blog_avg_total               0.485464\n",
       "last24h                      0.472061\n",
       "blog_avg_first24h            0.471999\n",
       "blog_median_last24h_tr       0.461627\n",
       "blog_std_difference          0.440003\n",
       "blog_std_24-48h              0.439152\n",
       "blog_std_last24h             0.433578\n",
       "blog_std_total               0.424616\n",
       "blog_std_first24h            0.384654\n",
       "blog_max_total               0.356604\n",
       "blog_median_total_tr         0.338961\n",
       "blog_avg_24-48h_tr           0.337775\n",
       "blog_avg_last24h_tr          0.335829\n",
       "blog_avg_first24h_tr         0.329670\n",
       "blog_avg_total_tr            0.328525\n",
       "blog_median_first24h_tr      0.323661\n",
       "blog_max_24-48h              0.322775\n",
       "blog_max_last24h             0.322106\n",
       "blog_max_difference          0.320133\n",
       "total                        0.314446\n",
       "first24h                     0.314177\n",
       "blog_max_first24h            0.299688\n",
       "difference                   0.296273\n",
       "blog_std_difference_tr       0.292805\n",
       "blog_std_24-48h_tr           0.285755\n",
       "blog_std_last24h_tr          0.283884\n",
       "blog_std_total_tr            0.266815\n",
       "blog_std_first24h_tr         0.265203\n",
       "last24h_tr                   0.260903\n",
       "blog_max_last24h_tr          0.251493\n",
       "blog_max_24-48h_tr           0.251485\n",
       "blog_max_total_tr            0.247457\n",
       "blog_max_difference_tr       0.245544\n",
       "blog_avg_difference_tr       0.233080\n",
       "blog_max_first24h_tr         0.232089\n",
       "first24h_tr                  0.198638\n",
       "total_tr                     0.191917\n",
       "difference_tr                0.146145\n",
       "24-48h                       0.117642\n",
       "word92                       0.080473\n",
       "24-48h_tr                    0.067141\n",
       "word184                      0.064753\n",
       "word5                        0.064112\n",
       "word96                       0.063923\n",
       "word170                      0.063903\n",
       "word39                       0.061460\n",
       "word7                        0.061238\n",
       "word81                       0.060322\n",
       "word148                      0.058703\n",
       "word186                      0.055606\n",
       "word132                      0.055318\n",
       "word77                       0.054750\n",
       "blog_min_first24h            0.053221\n",
       "blog_min_total               0.053221\n",
       "word164                      0.051995\n",
       "word122                      0.051541\n",
       "word40                       0.050907\n",
       "word151                      0.049974\n",
       "lenght_post                  0.048209\n",
       "word129                      0.045419\n",
       "word63                       0.045107\n",
       "word17                       0.044715\n",
       "word118                      0.043835\n",
       "word89                       0.043425\n",
       "word171                      0.041580\n",
       "word157                      0.040974\n",
       "word133                      0.038016\n",
       "word58                       0.037928\n",
       "word131                      0.035357\n",
       "word97                       0.035195\n",
       "blog_min_last24h             0.034916\n",
       "word140                      0.034695\n",
       "word2                        0.033752\n",
       "word15                       0.033374\n",
       "word159                      0.033300\n",
       "word60                       0.032340\n",
       "word10                       0.031312\n",
       "word145                      0.030371\n",
       "word179                      0.027677\n",
       "word34                       0.027294\n",
       "word135                      0.025934\n",
       "word53                       0.025293\n",
       "word52                       0.025017\n",
       "word168                      0.024109\n",
       "word121                      0.024025\n",
       "word75                       0.023639\n",
       "word165                      0.022007\n",
       "word6                        0.021817\n",
       "word185                      0.021600\n",
       "word74                       0.021484\n",
       "word79                       0.020781\n",
       "word72                       0.019945\n",
       "word134                      0.019839\n",
       "word199                      0.019466\n",
       "word42                       0.018614\n",
       "word109                      0.018127\n",
       "word76                       0.017458\n",
       "word190                      0.017291\n",
       "word172                      0.017250\n",
       "word73                       0.017025\n",
       "word66                       0.016475\n",
       "word144                      0.016079\n",
       "word26                       0.016067\n",
       "word166                      0.015934\n",
       "word101                      0.015695\n",
       "word156                      0.015629\n",
       "word78                       0.015366\n",
       "word146                      0.015329\n",
       "word54                       0.013581\n",
       "word143                      0.013384\n",
       "word195                      0.013151\n",
       "word192                      0.013099\n",
       "word23                       0.012943\n",
       "word154                      0.012197\n",
       "word124                      0.010315\n",
       "word30                       0.010222\n",
       "word119                      0.010158\n",
       "word41                       0.009785\n",
       "word112                      0.009739\n",
       "word27                       0.009347\n",
       "word193                      0.008334\n",
       "word113                      0.008176\n",
       "word100                      0.007944\n",
       "word48                       0.007736\n",
       "word127                      0.007250\n",
       "word36                       0.007120\n",
       "word153                      0.007086\n",
       "word147                      0.007064\n",
       "word56                       0.006732\n",
       "word114                      0.006725\n",
       "word57                       0.006716\n",
       "word125                      0.006525\n",
       "word43                       0.006345\n",
       "word130                      0.005843\n",
       "word102                      0.005558\n",
       "word198                      0.005246\n",
       "word91                       0.005214\n",
       "word13                       0.005094\n",
       "word88                       0.004966\n",
       "word64                       0.004865\n",
       "word152                      0.004798\n",
       "word65                       0.004748\n",
       "word1                        0.004429\n",
       "word163                      0.004345\n",
       "word46                       0.004174\n",
       "word197                      0.003610\n",
       "word183                      0.003577\n",
       "word55                       0.003400\n",
       "word38                       0.003389\n",
       "word108                      0.003139\n",
       "word71                       0.002858\n",
       "word28                       0.002787\n",
       "word20                       0.002537\n",
       "blog_median_difference_tr    0.002513\n",
       "word98                       0.002406\n",
       "blog_median_24-48h_tr        0.002224\n",
       "word62                       0.002174\n",
       "word59                       0.002005\n",
       "word37                       0.001608\n",
       "word21                       0.001515\n",
       "word196                      0.001495\n",
       "word8                        0.001491\n",
       "word149                      0.001472\n",
       "word51                       0.000949\n",
       "word137                      0.000862\n",
       "word175                      0.000618\n",
       "word12                       0.000457\n",
       "word169                      0.000237\n",
       "word120                      0.000228\n",
       "word80                       0.000096\n",
       "word176                     -0.000001\n",
       "word155                     -0.000077\n",
       "word29                      -0.000181\n",
       "word4                       -0.000268\n",
       "word178                     -0.000353\n",
       "word24                      -0.000449\n",
       "word160                     -0.000473\n",
       "word31                      -0.000574\n",
       "word187                     -0.000735\n",
       "word177                     -0.000786\n",
       "word44                      -0.000896\n",
       "word162                     -0.001137\n",
       "blog_min_first24h_tr        -0.001228\n",
       "blog_min_total_tr           -0.001228\n",
       "word194                     -0.001284\n",
       "avg_parent                  -0.001354\n",
       "word115                     -0.001527\n",
       "word18                      -0.001568\n",
       "word19                      -0.001568\n",
       "word150                     -0.001568\n",
       "word14                      -0.001568\n",
       "word188                     -0.001568\n",
       "word11                      -0.001568\n",
       "word181                     -0.001568\n",
       "word174                     -0.001568\n",
       "word128                     -0.001568\n",
       "word99                      -0.001568\n",
       "word35                      -0.001568\n",
       "word3                       -0.001568\n",
       "word94                      -0.001568\n",
       "word107                     -0.001568\n",
       "word110                     -0.001568\n",
       "word87                      -0.001568\n",
       "word32                      -0.001568\n",
       "word117                     -0.001568\n",
       "word104                     -0.001568\n",
       "word70                      -0.001568\n",
       "word136                     -0.001568\n",
       "word138                     -0.001568\n",
       "word68                      -0.001568\n",
       "word123                     -0.001769\n",
       "word61                      -0.001802\n",
       "word67                      -0.001811\n",
       "word33                      -0.001929\n",
       "word22                      -0.001966\n",
       "word49                      -0.001966\n",
       "word182                     -0.001986\n",
       "word103                     -0.001993\n",
       "word161                     -0.002030\n",
       "word93                      -0.002030\n",
       "word47                      -0.002074\n",
       "word106                     -0.002221\n",
       "word25                      -0.002259\n",
       "max_parent                  -0.002362\n",
       "word191                     -0.002369\n",
       "word86                      -0.002408\n",
       "word105                     -0.002411\n",
       "word95                      -0.002447\n",
       "word111                     -0.002469\n",
       "word167                     -0.002660\n",
       "word173                     -0.002725\n",
       "word180                     -0.002754\n",
       "word50                      -0.002985\n",
       "word141                     -0.003541\n",
       "word16                      -0.003553\n",
       "word116                     -0.003677\n",
       "word69                      -0.003723\n",
       "word45                      -0.003724\n",
       "word142                     -0.004113\n",
       "blog_median_difference      -0.004137\n",
       "word85                      -0.004241\n",
       "word82                      -0.004510\n",
       "word139                     -0.004516\n",
       "word126                     -0.005383\n",
       "word90                      -0.005481\n",
       "word189                     -0.005553\n",
       "word84                      -0.005726\n",
       "word9                       -0.005750\n",
       "word83                      -0.006695\n",
       "word158                     -0.006848\n",
       "time_first_post             -0.152908\n",
       "blog_min_difference_tr      -0.230493\n",
       "blog_min_difference         -0.280792\n",
       "blog_min_24-48h                   NaN\n",
       "blog_min_last24h_tr               NaN\n",
       "blog_min_24-48h_tr                NaN\n",
       "min_parent                        NaN\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = df_data.drop(df_data.iloc[:,261:277],axis=1).corr()\n",
    "corr_matrix.loc['target'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Train and Test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a test set at the beginning of the project avoid *data snooping* bias, i.e., \"when you estimate the generalization error using the test set, your estimate will be too optimistic, and you will launch a system that will not perform as well as expected\" (GÉRON, 2019).\n",
    "\n",
    "In this data set, the test set has already been divided. Therefore, we do not need to create a test set, just separete the target value from the other attributes to create our training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_X_train = df_data.drop('target',axis=1).copy()\n",
    "blog_y_train = df_data['target'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data for ML algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating the ML models, we need to prepare the data so that the ML algorithms will work properly.\n",
    "\n",
    "First, we need to clean missing values from the dataset. Second, we need to put all the attributes in the same scale because \"Machine Learning algorithms don’t perform well when the input numerical attributes have very different scales\" [(GÉRON, 2019)](https://www.amazon.com.br/Hands-Machine-Learning-Scikit-Learn-TensorFlow/dp/1492032646).\n",
    "\n",
    "We verify that there is no missing values in our data set. So, we just prepare a pipeline to do the scaling when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_X_train.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def estimator_transf(estimator):\n",
    "    pipeline = Pipeline(steps=[('m', estimator)])\n",
    "    return pipeline\n",
    "\n",
    "def estimator_scaler(estimator):\n",
    "    pipeline = Pipeline(steps=[('scaler',StandardScaler()),('model', estimator)])\n",
    "    return pipeline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Train ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing the data set, we are ready to select and train our ML model.\n",
    "\n",
    "We start with a Linear Regression (LR) model. \"A regression model, such as linear regression, models an output value based on a linear combination of input values\" [(BROWNLEE, 2020)](https://machinelearningmastery.com/introduction-to-time-series-forecasting-with-python/).\n",
    "\n",
    "Then, we try some regularized linear models. This kind of model constrain the weights of the model, avoiding overfitting (GÉRON, 2019). We try three regularized linear models [(BROWNLEE, 2016)](https://machinelearningmastery.com/machine-learning-with-python/):\n",
    "\n",
    "1. Ridge regression. This model model uses the L2 regularization. It adds “squared magnitude” of coefficient as a penalty term to the loss function [(NAGPAL, 2017)](https://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c).\n",
    "2. Lasso regression. This model model uses the L1 regularization. It adds “absolute value of magnitude” of coefficient as penalty term to the loss function (NAGPAL, 2017).\n",
    "3. Elastic Net. This model combines the Ridge and the Lasso models. \"It seeks to minimize the complexity of the regression model (magnitude and number of regression coefficients) by penalizing the model using both the L2-norm (sum squared coefficient values) and the L1-norm (sum absolute coefficient values)\" (BROWNLEE, 2016).\n",
    "\n",
    "Finally, we also try some nonlinear algorithms:\n",
    "\n",
    "1. Classification and Regression Trees (CART). It uses \"the train- ing data to select the best points to split the data in order to minimize a cost metric\" (BROWNLEE, 2016).\n",
    "2. k-Nearest Neighbors (KNN). This model \"locates the k most similar instances in the training dataset for a new data instance\" (BROWNLEE, 2016).\n",
    "\n",
    "The models are evaluated using the mean absolute error (MAE), root square mean error (RMSE), and R². RMSE punish larger errors more than smaller errors, inflating or magnifying the mean error score. This is due to the square of the error value. MAE does not give more or less weight to different types of errors and instead the scores increase linearly with increases in error. MAE is the simplest evaluation metric and most easily interpreted. R² tells you how much variance your model accounts for. In the case of the MAE and RMSE, the lower the better. But for R², the close the value is to 1, the better ([HALE, 2020](https://towardsdatascience.com/which-evaluation-metric-should-you-use-in-machine-learning-regression-problems-20cdaef258e); [BROWNLEE, 2021](https://machinelearningmastery.com/regression-metrics-for-machine-learning/)).\n",
    "\n",
    "Besides, \"the key to a fair comparison of machine learning algorithms is ensuring that each algorithm is evaluated in the same way on the same data. You can achieve this by forcing each algorithm to be evaluated on a consistent test harness\" (BROWNLEE, 2016). In this project, we do this by using the same split in the cross validation. We use the KFold function from the sklearn library with a random value rs as the random_state parameter. Although the rs value change everytime the notebook is run, once it is set, the same rs value is used in all the models. This guarantees that all the models are evaluated on the same data.\n",
    "\n",
    "The result of the tests of the models with the training data shows that **the KNN is the best model**. It has the lowest MAE and RMSE, and the highest R².\n",
    "\n",
    "However, differing scales of the raw data could be negatively impacting the performance of some of the models. Therefore, we test the models again, but this time we standardize the data set.\n",
    "\n",
    "We can see that the performance of most models improved with standardization. However, the performance of the KNN degraded with the standardized data. Even so, KNN was still the best method.\n",
    "\n",
    "**Therefore, for this initial test, we verify that KNN without standardization is the best model for our data**.\n",
    "\n",
    "However, **using the data set with all the 280 attributes requires a lot of computing time**. So, let's try some featuring selection methods to see if we can reduce the number of attributes to be used in our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def estimator_cross_val (model,estimator,pipe,matriz,rs,X,y):\n",
    "    pipe_ = pipe(estimator)\n",
    "    scoring = ['neg_mean_absolute_error', 'neg_root_mean_squared_error','r2']\n",
    "    kfold = KFold(n_splits=5, random_state=rs,shuffle=True)\n",
    "    scores = cross_validate(pipe_,X,y,cv=kfold,scoring=scoring)\n",
    "    \n",
    "    mae_scores = -scores.get('test_neg_mean_absolute_error')\n",
    "    mae_mean = mae_scores.mean()\n",
    "    mae_std = mae_scores.std()\n",
    "    \n",
    "    rmse_scores = -scores.get('test_neg_root_mean_squared_error')\n",
    "    rmse_mean = rmse_scores.mean()\n",
    "    rmse_std = rmse_scores.std()\n",
    "    \n",
    "    r2_scores = scores.get('test_r2')\n",
    "    r2_mean = r2_scores.mean()\n",
    "    r2_std = r2_scores.std()\n",
    "    \n",
    "    results_ = [model,mae_mean,mae_std,rmse_mean,rmse_std,r2_mean,r2_std]\n",
    "    results_ = pd.Series(results_, index = matriz.columns)\n",
    "    results = matriz.append(results_,ignore_index=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE_mean</th>\n",
       "      <th>MAE_std</th>\n",
       "      <th>RMSE_mean</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>R2_mean</th>\n",
       "      <th>R2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.535883</td>\n",
       "      <td>0.155004</td>\n",
       "      <td>30.383434</td>\n",
       "      <td>1.101252</td>\n",
       "      <td>0.347938</td>\n",
       "      <td>0.030813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.530001</td>\n",
       "      <td>0.155896</td>\n",
       "      <td>30.378057</td>\n",
       "      <td>1.104894</td>\n",
       "      <td>0.348174</td>\n",
       "      <td>0.030823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.122793</td>\n",
       "      <td>0.183914</td>\n",
       "      <td>30.302539</td>\n",
       "      <td>1.137340</td>\n",
       "      <td>0.351477</td>\n",
       "      <td>0.030541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.135748</td>\n",
       "      <td>0.185702</td>\n",
       "      <td>30.309116</td>\n",
       "      <td>1.139229</td>\n",
       "      <td>0.351195</td>\n",
       "      <td>0.030631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>6.387953</td>\n",
       "      <td>0.228165</td>\n",
       "      <td>28.589911</td>\n",
       "      <td>1.318047</td>\n",
       "      <td>0.423054</td>\n",
       "      <td>0.028169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CART</td>\n",
       "      <td>6.373964</td>\n",
       "      <td>0.214273</td>\n",
       "      <td>33.118618</td>\n",
       "      <td>1.396891</td>\n",
       "      <td>0.220945</td>\n",
       "      <td>0.091865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  MAE_mean   MAE_std  RMSE_mean  RMSE_std   R2_mean  \\\n",
       "0  Linear Regression  9.535883  0.155004  30.383434  1.101252  0.347938   \n",
       "1   Ridge Regression  9.530001  0.155896  30.378057  1.104894  0.348174   \n",
       "2              Lasso  9.122793  0.183914  30.302539  1.137340  0.351477   \n",
       "3        Elastic Net  9.135748  0.185702  30.309116  1.139229  0.351195   \n",
       "4                KNN  6.387953  0.228165  28.589911  1.318047  0.423054   \n",
       "5               CART  6.373964  0.214273  33.118618  1.396891  0.220945   \n",
       "\n",
       "     R2_std  \n",
       "0  0.030813  \n",
       "1  0.030823  \n",
       "2  0.030541  \n",
       "3  0.030631  \n",
       "4  0.028169  \n",
       "5  0.091865  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randrange\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "rs = randrange(10000)\n",
    "matriz = pd.DataFrame(columns=['model','MAE_mean','MAE_std','RMSE_mean','RMSE_std','R2_mean','R2_std'])\n",
    "\n",
    "matriz = estimator_cross_val('Linear Regression',LinearRegression(),estimator_transf,matriz,rs,blog_X_train,blog_y_train)\n",
    "matriz = estimator_cross_val('Ridge Regression',Ridge(),estimator_transf,matriz,rs,blog_X_train,blog_y_train)\n",
    "matriz = estimator_cross_val('Lasso',Lasso(),estimator_transf,matriz,rs,blog_X_train,blog_y_train)\n",
    "matriz = estimator_cross_val('Elastic Net',ElasticNet(),estimator_transf,matriz,rs,blog_X_train,blog_y_train)\n",
    "matriz = estimator_cross_val('KNN',KNeighborsRegressor(),estimator_transf,matriz,rs,blog_X_train,blog_y_train)\n",
    "matriz = estimator_cross_val('CART',DecisionTreeRegressor(),estimator_transf,matriz,rs,blog_X_train,blog_y_train)\n",
    "matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MAE_mean</th>\n",
       "      <th>MAE_std</th>\n",
       "      <th>RMSE_mean</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>R2_mean</th>\n",
       "      <th>R2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.536043</td>\n",
       "      <td>0.158151</td>\n",
       "      <td>30.382693</td>\n",
       "      <td>1.103323</td>\n",
       "      <td>0.347973</td>\n",
       "      <td>0.030809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.535920</td>\n",
       "      <td>0.156277</td>\n",
       "      <td>30.380541</td>\n",
       "      <td>1.102180</td>\n",
       "      <td>0.348060</td>\n",
       "      <td>0.030874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>8.435559</td>\n",
       "      <td>0.157798</td>\n",
       "      <td>30.499635</td>\n",
       "      <td>1.170340</td>\n",
       "      <td>0.343283</td>\n",
       "      <td>0.025468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>8.448594</td>\n",
       "      <td>0.167776</td>\n",
       "      <td>30.672138</td>\n",
       "      <td>1.181716</td>\n",
       "      <td>0.335904</td>\n",
       "      <td>0.024106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNN</td>\n",
       "      <td>6.820830</td>\n",
       "      <td>0.147086</td>\n",
       "      <td>29.504949</td>\n",
       "      <td>1.002954</td>\n",
       "      <td>0.384447</td>\n",
       "      <td>0.038376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CART</td>\n",
       "      <td>6.379402</td>\n",
       "      <td>0.323984</td>\n",
       "      <td>32.991296</td>\n",
       "      <td>1.777952</td>\n",
       "      <td>0.228157</td>\n",
       "      <td>0.086245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               model  MAE_mean   MAE_std  RMSE_mean  RMSE_std   R2_mean  \\\n",
       "0  Linear Regression  9.536043  0.158151  30.382693  1.103323  0.347973   \n",
       "1   Ridge Regression  9.535920  0.156277  30.380541  1.102180  0.348060   \n",
       "2              Lasso  8.435559  0.157798  30.499635  1.170340  0.343283   \n",
       "3        Elastic Net  8.448594  0.167776  30.672138  1.181716  0.335904   \n",
       "4                KNN  6.820830  0.147086  29.504949  1.002954  0.384447   \n",
       "5               CART  6.379402  0.323984  32.991296  1.777952  0.228157   \n",
       "\n",
       "     R2_std  \n",
       "0  0.030809  \n",
       "1  0.030874  \n",
       "2  0.025468  \n",
       "3  0.024106  \n",
       "4  0.038376  \n",
       "5  0.086245  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz2 = pd.DataFrame(columns=['model','MAE_mean','MAE_std','RMSE_mean','RMSE_std','R2_mean','R2_std'])\n",
    "\n",
    "matriz2 = estimator_cross_val('Linear Regression',LinearRegression(),estimator_scaler,matriz2,rs,blog_X_train,blog_y_train)\n",
    "matriz2 = estimator_cross_val('Ridge Regression',Ridge(),estimator_scaler,matriz2,rs,blog_X_train,blog_y_train)\n",
    "matriz2 = estimator_cross_val('Lasso',Lasso(),estimator_scaler,matriz2,rs,blog_X_train,blog_y_train)\n",
    "matriz2 = estimator_cross_val('Elastic Net',ElasticNet(),estimator_scaler,matriz2,rs,blog_X_train,blog_y_train)\n",
    "matriz2 = estimator_cross_val('KNN',KNeighborsRegressor(),estimator_scaler,matriz2,rs,blog_X_train,blog_y_train)\n",
    "matriz2 = estimator_cross_val('CART',DecisionTreeRegressor(),estimator_scaler,matriz2,rs,blog_X_train,blog_y_train)\n",
    "matriz2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Feature selection is the process of reducing the number of input variables when developing a predictive model. It is desirable to reduce the number of input variables to both reduce the computational cost of modeling and, in many cases, to improve the performance of the model\" (BROWNLEE, 2021).\n",
    "\n",
    "There are two main techniques of feature selection: supervised and unsupervised. Supervised methods use the target variable, while unsupervised methods do not (BROWNLEE, 2021).\n",
    "\n",
    "Besides, the supervised techniques can be divided in (BROWNLEE, 2021):\n",
    "\n",
    "1. Intrinsic: Algorithms that perform automatic feature selection during training.\n",
    "2. Wrapper: Search subsets of features that perform according to a predictive model.\n",
    "3. Filter: Select subsets of features based on their relationship with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the methods of feature selection are more appropriated for numerical variables and others for categorical ones. One popular feature selection techniques used for both numerical variables and categorical variable is Mutual Information Statistics (BROWNLEE, 2021). \n",
    "\n",
    "\"Mutual information from the field of information theory is the application of information gain (typically used in the construction of decision trees) to feature selection. Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable\"  (BROWNLEE, 2021).\n",
    "\n",
    "We find that many attributes have negligible information value. 181 features have a contribution score over 0.0001, 144 over 0.001, 65 over 0.01, and only 33 over 0.1. **These numbers can vary depending on the training set**. Therefore, we will test the 30, 70, 150, and 190 best features and compare it with the results obtained using all features. \n",
    "\n",
    "**We see that the performance using the 70, 150, and 190 best features are almost the same of using all features. Using the 30 beast features is just slighlty worst than using all features**. Moreover, in all cases the KNN model have the best performance.\n",
    "\n",
    "We could do a grid search to \"systematically test a range of different numbers of selected features and discover which results in the best performing model\" (BROWNLEE, 2021). **However, a grid search to determine the optimum number of features would require a lot of computing time and the benefit would not be significant in our evaluation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# feature selection\n",
    "def select_features(X_train, y_train,k_):\n",
    "    # configure to select all features\n",
    "    fs = SelectKBest(score_func=mutual_info_classif, k=k_) \n",
    "    # learn relationship from training data\n",
    "    fs.fit(X_train, y_train)\n",
    "    # transform train input data\n",
    "    X_train_fs = fs.transform(X_train)\n",
    "    return X_train_fs, fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score    192\n",
      "dtype: int64\n",
      "Score    164\n",
      "dtype: int64\n",
      "Score    62\n",
      "dtype: int64\n",
      "Score    33\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# feature selection\n",
    "blog_X_train_mi, mi = select_features(blog_X_train, blog_y_train,'all')\n",
    "\n",
    "# what are scores for the features\n",
    "MI = pd.DataFrame(mi.scores_, columns = ['Score'])\n",
    "\n",
    "print(MI[MI > 0.0001].count())\n",
    "print(MI[MI > 0.001].count())\n",
    "print(MI[MI > 0.01].count())\n",
    "print(MI[MI > 0.1].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>model</th>\n",
       "      <th>MAE_mean</th>\n",
       "      <th>MAE_std</th>\n",
       "      <th>RMSE_mean</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>R2_mean</th>\n",
       "      <th>R2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>8.088844</td>\n",
       "      <td>0.191775</td>\n",
       "      <td>30.478789</td>\n",
       "      <td>1.097408</td>\n",
       "      <td>0.343845</td>\n",
       "      <td>0.030700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>8.088783</td>\n",
       "      <td>0.190884</td>\n",
       "      <td>30.478051</td>\n",
       "      <td>1.099127</td>\n",
       "      <td>0.343878</td>\n",
       "      <td>0.030730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>8.092827</td>\n",
       "      <td>0.174670</td>\n",
       "      <td>30.435213</td>\n",
       "      <td>1.164739</td>\n",
       "      <td>0.345830</td>\n",
       "      <td>0.030713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>8.105448</td>\n",
       "      <td>0.176620</td>\n",
       "      <td>30.441817</td>\n",
       "      <td>1.155065</td>\n",
       "      <td>0.345539</td>\n",
       "      <td>0.030518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.475607</td>\n",
       "      <td>0.228648</td>\n",
       "      <td>28.754414</td>\n",
       "      <td>1.411436</td>\n",
       "      <td>0.414096</td>\n",
       "      <td>0.061922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>CART</td>\n",
       "      <td>7.132292</td>\n",
       "      <td>0.190374</td>\n",
       "      <td>34.518267</td>\n",
       "      <td>1.294679</td>\n",
       "      <td>0.158520</td>\n",
       "      <td>0.038165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.247438</td>\n",
       "      <td>0.197738</td>\n",
       "      <td>30.344834</td>\n",
       "      <td>1.113804</td>\n",
       "      <td>0.349604</td>\n",
       "      <td>0.030980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.246929</td>\n",
       "      <td>0.199456</td>\n",
       "      <td>30.342582</td>\n",
       "      <td>1.115527</td>\n",
       "      <td>0.349703</td>\n",
       "      <td>0.030987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.120361</td>\n",
       "      <td>0.185669</td>\n",
       "      <td>30.308663</td>\n",
       "      <td>1.147322</td>\n",
       "      <td>0.351221</td>\n",
       "      <td>0.030775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.133710</td>\n",
       "      <td>0.187704</td>\n",
       "      <td>30.310683</td>\n",
       "      <td>1.145248</td>\n",
       "      <td>0.351125</td>\n",
       "      <td>0.030897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>70</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.400156</td>\n",
       "      <td>0.216946</td>\n",
       "      <td>28.595075</td>\n",
       "      <td>1.320200</td>\n",
       "      <td>0.422814</td>\n",
       "      <td>0.028841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.515588</td>\n",
       "      <td>0.416096</td>\n",
       "      <td>33.741221</td>\n",
       "      <td>3.170468</td>\n",
       "      <td>0.193697</td>\n",
       "      <td>0.119113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>150</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.363710</td>\n",
       "      <td>0.171935</td>\n",
       "      <td>30.352465</td>\n",
       "      <td>1.108021</td>\n",
       "      <td>0.349276</td>\n",
       "      <td>0.030831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>150</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.362735</td>\n",
       "      <td>0.172976</td>\n",
       "      <td>30.348999</td>\n",
       "      <td>1.110113</td>\n",
       "      <td>0.349427</td>\n",
       "      <td>0.030846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.120361</td>\n",
       "      <td>0.185669</td>\n",
       "      <td>30.308663</td>\n",
       "      <td>1.147322</td>\n",
       "      <td>0.351221</td>\n",
       "      <td>0.030775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.133621</td>\n",
       "      <td>0.187782</td>\n",
       "      <td>30.310841</td>\n",
       "      <td>1.144982</td>\n",
       "      <td>0.351118</td>\n",
       "      <td>0.030895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>150</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.398503</td>\n",
       "      <td>0.215911</td>\n",
       "      <td>28.586372</td>\n",
       "      <td>1.309061</td>\n",
       "      <td>0.423163</td>\n",
       "      <td>0.028495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>150</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.460412</td>\n",
       "      <td>0.451631</td>\n",
       "      <td>33.906332</td>\n",
       "      <td>2.693713</td>\n",
       "      <td>0.187641</td>\n",
       "      <td>0.092196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>190</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.417921</td>\n",
       "      <td>0.175741</td>\n",
       "      <td>30.362591</td>\n",
       "      <td>1.110078</td>\n",
       "      <td>0.348824</td>\n",
       "      <td>0.031273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>190</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.416113</td>\n",
       "      <td>0.176636</td>\n",
       "      <td>30.358579</td>\n",
       "      <td>1.112333</td>\n",
       "      <td>0.348999</td>\n",
       "      <td>0.031295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>190</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.120417</td>\n",
       "      <td>0.185649</td>\n",
       "      <td>30.308688</td>\n",
       "      <td>1.147326</td>\n",
       "      <td>0.351220</td>\n",
       "      <td>0.030774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>190</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.133701</td>\n",
       "      <td>0.187659</td>\n",
       "      <td>30.310884</td>\n",
       "      <td>1.144988</td>\n",
       "      <td>0.351117</td>\n",
       "      <td>0.030894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>190</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.389682</td>\n",
       "      <td>0.223080</td>\n",
       "      <td>28.587145</td>\n",
       "      <td>1.320775</td>\n",
       "      <td>0.423153</td>\n",
       "      <td>0.028519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>190</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.386770</td>\n",
       "      <td>0.368084</td>\n",
       "      <td>33.523654</td>\n",
       "      <td>2.104213</td>\n",
       "      <td>0.203756</td>\n",
       "      <td>0.092511</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features              model  MAE_mean   MAE_std  RMSE_mean  RMSE_std  \\\n",
       "0        30  Linear Regression  8.088844  0.191775  30.478789  1.097408   \n",
       "1        30   Ridge Regression  8.088783  0.190884  30.478051  1.099127   \n",
       "2        30              Lasso  8.092827  0.174670  30.435213  1.164739   \n",
       "3        30        Elastic Net  8.105448  0.176620  30.441817  1.155065   \n",
       "4        30                KNN  6.475607  0.228648  28.754414  1.411436   \n",
       "5        30               CART  7.132292  0.190374  34.518267  1.294679   \n",
       "6        70  Linear Regression  9.247438  0.197738  30.344834  1.113804   \n",
       "7        70   Ridge Regression  9.246929  0.199456  30.342582  1.115527   \n",
       "8        70              Lasso  9.120361  0.185669  30.308663  1.147322   \n",
       "9        70        Elastic Net  9.133710  0.187704  30.310683  1.145248   \n",
       "10       70                KNN  6.400156  0.216946  28.595075  1.320200   \n",
       "11       70               CART  6.515588  0.416096  33.741221  3.170468   \n",
       "12      150  Linear Regression  9.363710  0.171935  30.352465  1.108021   \n",
       "13      150   Ridge Regression  9.362735  0.172976  30.348999  1.110113   \n",
       "14      150              Lasso  9.120361  0.185669  30.308663  1.147322   \n",
       "15      150        Elastic Net  9.133621  0.187782  30.310841  1.144982   \n",
       "16      150                KNN  6.398503  0.215911  28.586372  1.309061   \n",
       "17      150               CART  6.460412  0.451631  33.906332  2.693713   \n",
       "18      190  Linear Regression  9.417921  0.175741  30.362591  1.110078   \n",
       "19      190   Ridge Regression  9.416113  0.176636  30.358579  1.112333   \n",
       "20      190              Lasso  9.120417  0.185649  30.308688  1.147326   \n",
       "21      190        Elastic Net  9.133701  0.187659  30.310884  1.144988   \n",
       "22      190                KNN  6.389682  0.223080  28.587145  1.320775   \n",
       "23      190               CART  6.386770  0.368084  33.523654  2.104213   \n",
       "\n",
       "     R2_mean    R2_std  \n",
       "0   0.343845  0.030700  \n",
       "1   0.343878  0.030730  \n",
       "2   0.345830  0.030713  \n",
       "3   0.345539  0.030518  \n",
       "4   0.414096  0.061922  \n",
       "5   0.158520  0.038165  \n",
       "6   0.349604  0.030980  \n",
       "7   0.349703  0.030987  \n",
       "8   0.351221  0.030775  \n",
       "9   0.351125  0.030897  \n",
       "10  0.422814  0.028841  \n",
       "11  0.193697  0.119113  \n",
       "12  0.349276  0.030831  \n",
       "13  0.349427  0.030846  \n",
       "14  0.351221  0.030775  \n",
       "15  0.351118  0.030895  \n",
       "16  0.423163  0.028495  \n",
       "17  0.187641  0.092196  \n",
       "18  0.348824  0.031273  \n",
       "19  0.348999  0.031295  \n",
       "20  0.351220  0.030774  \n",
       "21  0.351117  0.030894  \n",
       "22  0.423153  0.028519  \n",
       "23  0.203756  0.092511  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimator_cross_val_fea (k,model,estimator,pipe,matriz,rs,X,y):\n",
    "    pipe_ = pipe(estimator)\n",
    "    scoring = ['neg_mean_absolute_error', 'neg_root_mean_squared_error','r2']\n",
    "    kfold = KFold(n_splits=5, random_state=rs,shuffle=True)\n",
    "    scores = cross_validate(pipe_,X,y,cv=kfold,scoring=scoring)\n",
    "    \n",
    "    mae_scores = -scores.get('test_neg_mean_absolute_error')\n",
    "    mae_mean = mae_scores.mean()\n",
    "    mae_std = mae_scores.std()\n",
    "    \n",
    "    rmse_scores = -scores.get('test_neg_root_mean_squared_error')\n",
    "    rmse_mean = rmse_scores.mean()\n",
    "    rmse_std = rmse_scores.std()\n",
    "    \n",
    "    r2_scores = scores.get('test_r2')\n",
    "    r2_mean = r2_scores.mean()\n",
    "    r2_std = r2_scores.std()\n",
    "    \n",
    "    results_ = [k,model,mae_mean,mae_std,rmse_mean,rmse_std,r2_mean,r2_std]\n",
    "    results_ = pd.Series(results_, index = matriz.columns)\n",
    "    results = matriz.append(results_,ignore_index=True)\n",
    "    return results\n",
    "\n",
    "matriz_mi = pd.DataFrame(columns=['features','model','MAE_mean','MAE_std','RMSE_mean','RMSE_std','R2_mean','R2_std'])\n",
    "\n",
    "for k in [30,70,150,190]:\n",
    "\n",
    "    best_features_mi = MI.transpose()\n",
    "    best_features_mi.columns = blog_X_train.columns\n",
    "    best_features_mi.sort_values('Score',axis=1,ascending=False,inplace=True)\n",
    "    best_features_mi.drop(best_features_mi.iloc[:,k:],axis=1,inplace=True)\n",
    "    blog_X_train_mi = blog_X_train[best_features_mi.columns]\n",
    "  \n",
    "    matriz_mi = estimator_cross_val_fea(k,'Linear Regression',LinearRegression(),     estimator_transf,matriz_mi,rs,blog_X_train_mi,blog_y_train)\n",
    "    matriz_mi = estimator_cross_val_fea(k,'Ridge Regression', Ridge(),                estimator_transf,matriz_mi,rs,blog_X_train_mi,blog_y_train)\n",
    "    matriz_mi = estimator_cross_val_fea(k,'Lasso',            Lasso(),                estimator_transf,matriz_mi,rs,blog_X_train_mi,blog_y_train)\n",
    "    matriz_mi = estimator_cross_val_fea(k,'Elastic Net',      ElasticNet(),           estimator_transf,matriz_mi,rs,blog_X_train_mi,blog_y_train)\n",
    "    matriz_mi = estimator_cross_val_fea(k,'KNN',              KNeighborsRegressor(),  estimator_transf,matriz_mi,rs,blog_X_train_mi,blog_y_train)\n",
    "    matriz_mi = estimator_cross_val_fea(k,'CART',             DecisionTreeRegressor(),estimator_transf,matriz_mi,rs,blog_X_train_mi,blog_y_train)\n",
    "\n",
    "matriz_mi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapper feature selection method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to handle data sets that combines numerical and categorical variables is to use a wrapper method. Some ofent used wrapper methods are Tree-Searching Methods, Stochastic Global Search, Step-Wise Models, and Recursive Feature Elimination (BROWNLEE, 2021).\n",
    "\n",
    "We use the **Recursive Feature Elimination (RFE) method**. This method searches \"for a subset of features by starting with all features in the training dataset and successfully removing features until the desired number remains. This is achieved by fitting the given machine learning algorithm used in the core of the model, ranking features by importance, discarding the least important features, and re-fitting the model\" (BROWNLEE, 2021).\n",
    "\n",
    "We use the RFE to reduce the attributes of the data set. We use the same number of features as in the Mutual information selection method. Thus, we select the 190, 150, 70, and 30 most relevant features and evaluate the models again. \n",
    "\n",
    "**We can see that the best model is the KNN with 30 features**. Moreover, the models performed better with the features selected using the RFE than with the ones selected using the mutual information. This model even performed better than the one using all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>model</th>\n",
       "      <th>MAE_mean</th>\n",
       "      <th>MAE_std</th>\n",
       "      <th>RMSE_mean</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>R2_mean</th>\n",
       "      <th>R2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.182049</td>\n",
       "      <td>0.189199</td>\n",
       "      <td>30.346092</td>\n",
       "      <td>1.128043</td>\n",
       "      <td>0.349626</td>\n",
       "      <td>0.029745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.182005</td>\n",
       "      <td>0.189197</td>\n",
       "      <td>30.346085</td>\n",
       "      <td>1.128039</td>\n",
       "      <td>0.349627</td>\n",
       "      <td>0.029745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.087821</td>\n",
       "      <td>0.193765</td>\n",
       "      <td>30.345144</td>\n",
       "      <td>1.146574</td>\n",
       "      <td>0.349707</td>\n",
       "      <td>0.029621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.116603</td>\n",
       "      <td>0.192219</td>\n",
       "      <td>30.347987</td>\n",
       "      <td>1.150661</td>\n",
       "      <td>0.349599</td>\n",
       "      <td>0.029488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.382809</td>\n",
       "      <td>0.224521</td>\n",
       "      <td>28.510587</td>\n",
       "      <td>1.247672</td>\n",
       "      <td>0.426304</td>\n",
       "      <td>0.024114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.495443</td>\n",
       "      <td>0.336321</td>\n",
       "      <td>34.154650</td>\n",
       "      <td>1.970775</td>\n",
       "      <td>0.174276</td>\n",
       "      <td>0.081794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.303662</td>\n",
       "      <td>0.168855</td>\n",
       "      <td>30.313320</td>\n",
       "      <td>1.137163</td>\n",
       "      <td>0.350973</td>\n",
       "      <td>0.031427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.303296</td>\n",
       "      <td>0.168870</td>\n",
       "      <td>30.313111</td>\n",
       "      <td>1.137050</td>\n",
       "      <td>0.350982</td>\n",
       "      <td>0.031425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.101846</td>\n",
       "      <td>0.176443</td>\n",
       "      <td>30.289602</td>\n",
       "      <td>1.146506</td>\n",
       "      <td>0.352024</td>\n",
       "      <td>0.031049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.117699</td>\n",
       "      <td>0.176200</td>\n",
       "      <td>30.288292</td>\n",
       "      <td>1.146147</td>\n",
       "      <td>0.352074</td>\n",
       "      <td>0.031171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>70</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.357464</td>\n",
       "      <td>0.214932</td>\n",
       "      <td>28.582853</td>\n",
       "      <td>1.256152</td>\n",
       "      <td>0.423265</td>\n",
       "      <td>0.026898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.307998</td>\n",
       "      <td>0.143808</td>\n",
       "      <td>33.180574</td>\n",
       "      <td>1.747371</td>\n",
       "      <td>0.216566</td>\n",
       "      <td>0.113491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>150</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.430689</td>\n",
       "      <td>0.173049</td>\n",
       "      <td>30.367247</td>\n",
       "      <td>1.117199</td>\n",
       "      <td>0.348665</td>\n",
       "      <td>0.030680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>150</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.427394</td>\n",
       "      <td>0.174786</td>\n",
       "      <td>30.366983</td>\n",
       "      <td>1.117896</td>\n",
       "      <td>0.348679</td>\n",
       "      <td>0.030649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.139907</td>\n",
       "      <td>0.183615</td>\n",
       "      <td>30.307162</td>\n",
       "      <td>1.136086</td>\n",
       "      <td>0.351276</td>\n",
       "      <td>0.030583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.158194</td>\n",
       "      <td>0.186132</td>\n",
       "      <td>30.314601</td>\n",
       "      <td>1.132900</td>\n",
       "      <td>0.350944</td>\n",
       "      <td>0.030752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>150</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.386918</td>\n",
       "      <td>0.223945</td>\n",
       "      <td>28.586857</td>\n",
       "      <td>1.319133</td>\n",
       "      <td>0.423165</td>\n",
       "      <td>0.028444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>150</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.360462</td>\n",
       "      <td>0.193760</td>\n",
       "      <td>32.738433</td>\n",
       "      <td>2.158942</td>\n",
       "      <td>0.239203</td>\n",
       "      <td>0.105038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>190</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.502598</td>\n",
       "      <td>0.165032</td>\n",
       "      <td>30.363019</td>\n",
       "      <td>1.119150</td>\n",
       "      <td>0.348848</td>\n",
       "      <td>0.030699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>190</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.497262</td>\n",
       "      <td>0.164294</td>\n",
       "      <td>30.360802</td>\n",
       "      <td>1.118455</td>\n",
       "      <td>0.348941</td>\n",
       "      <td>0.030711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>190</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.111321</td>\n",
       "      <td>0.186549</td>\n",
       "      <td>30.324333</td>\n",
       "      <td>1.155870</td>\n",
       "      <td>0.350547</td>\n",
       "      <td>0.031129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>190</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.132434</td>\n",
       "      <td>0.187574</td>\n",
       "      <td>30.324661</td>\n",
       "      <td>1.145183</td>\n",
       "      <td>0.350512</td>\n",
       "      <td>0.031206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>190</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.388018</td>\n",
       "      <td>0.228146</td>\n",
       "      <td>28.589889</td>\n",
       "      <td>1.318046</td>\n",
       "      <td>0.423055</td>\n",
       "      <td>0.028165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>190</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.407348</td>\n",
       "      <td>0.277244</td>\n",
       "      <td>33.224163</td>\n",
       "      <td>1.969485</td>\n",
       "      <td>0.216854</td>\n",
       "      <td>0.094667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features              model  MAE_mean   MAE_std  RMSE_mean  RMSE_std  \\\n",
       "0        30  Linear Regression  9.182049  0.189199  30.346092  1.128043   \n",
       "1        30   Ridge Regression  9.182005  0.189197  30.346085  1.128039   \n",
       "2        30              Lasso  9.087821  0.193765  30.345144  1.146574   \n",
       "3        30        Elastic Net  9.116603  0.192219  30.347987  1.150661   \n",
       "4        30                KNN  6.382809  0.224521  28.510587  1.247672   \n",
       "5        30               CART  6.495443  0.336321  34.154650  1.970775   \n",
       "6        70  Linear Regression  9.303662  0.168855  30.313320  1.137163   \n",
       "7        70   Ridge Regression  9.303296  0.168870  30.313111  1.137050   \n",
       "8        70              Lasso  9.101846  0.176443  30.289602  1.146506   \n",
       "9        70        Elastic Net  9.117699  0.176200  30.288292  1.146147   \n",
       "10       70                KNN  6.357464  0.214932  28.582853  1.256152   \n",
       "11       70               CART  6.307998  0.143808  33.180574  1.747371   \n",
       "12      150  Linear Regression  9.430689  0.173049  30.367247  1.117199   \n",
       "13      150   Ridge Regression  9.427394  0.174786  30.366983  1.117896   \n",
       "14      150              Lasso  9.139907  0.183615  30.307162  1.136086   \n",
       "15      150        Elastic Net  9.158194  0.186132  30.314601  1.132900   \n",
       "16      150                KNN  6.386918  0.223945  28.586857  1.319133   \n",
       "17      150               CART  6.360462  0.193760  32.738433  2.158942   \n",
       "18      190  Linear Regression  9.502598  0.165032  30.363019  1.119150   \n",
       "19      190   Ridge Regression  9.497262  0.164294  30.360802  1.118455   \n",
       "20      190              Lasso  9.111321  0.186549  30.324333  1.155870   \n",
       "21      190        Elastic Net  9.132434  0.187574  30.324661  1.145183   \n",
       "22      190                KNN  6.388018  0.228146  28.589889  1.318046   \n",
       "23      190               CART  6.407348  0.277244  33.224163  1.969485   \n",
       "\n",
       "     R2_mean    R2_std  \n",
       "0   0.349626  0.029745  \n",
       "1   0.349627  0.029745  \n",
       "2   0.349707  0.029621  \n",
       "3   0.349599  0.029488  \n",
       "4   0.426304  0.024114  \n",
       "5   0.174276  0.081794  \n",
       "6   0.350973  0.031427  \n",
       "7   0.350982  0.031425  \n",
       "8   0.352024  0.031049  \n",
       "9   0.352074  0.031171  \n",
       "10  0.423265  0.026898  \n",
       "11  0.216566  0.113491  \n",
       "12  0.348665  0.030680  \n",
       "13  0.348679  0.030649  \n",
       "14  0.351276  0.030583  \n",
       "15  0.350944  0.030752  \n",
       "16  0.423165  0.028444  \n",
       "17  0.239203  0.105038  \n",
       "18  0.348848  0.030699  \n",
       "19  0.348941  0.030711  \n",
       "20  0.350547  0.031129  \n",
       "21  0.350512  0.031206  \n",
       "22  0.423055  0.028165  \n",
       "23  0.216854  0.094667  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "matriz_rfe = pd.DataFrame(columns=['features','model','MAE_mean','MAE_std','RMSE_mean','RMSE_std','R2_mean','R2_std'])\n",
    "rfe_features = pd.DataFrame()\n",
    "\n",
    "for k in [30,70,150,190]:\n",
    "    rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=k)\n",
    "    rfe.fit(blog_X_train,blog_y_train)\n",
    "    RF = pd.DataFrame(rfe.support_, columns = ['{} Features'.format(k)])\n",
    "    rfe_features['{} Features'.format(k)] = RF['{} Features'.format(k)]\n",
    "    \n",
    "    best_features_rfe = RF.transpose()\n",
    "    best_features_rfe.columns = blog_X_train.columns\n",
    "    best_features_rfe.sort_values('{} Features'.format(k),axis=1,ascending=False,inplace=True)\n",
    "    best_features_rfe.drop(best_features_rfe.iloc[:,k:],axis=1,inplace=True)\n",
    "    blog_X_train_rfe = blog_X_train[best_features_rfe.columns]\n",
    "    blog_X_train_rfe.head()\n",
    "  \n",
    "    matriz_rfe = estimator_cross_val_fea(k,'Linear Regression',LinearRegression(),     estimator_transf,matriz_rfe,rs,blog_X_train_rfe,blog_y_train)\n",
    "    matriz_rfe = estimator_cross_val_fea(k,'Ridge Regression', Ridge(),                estimator_transf,matriz_rfe,rs,blog_X_train_rfe,blog_y_train)\n",
    "    matriz_rfe = estimator_cross_val_fea(k,'Lasso',            Lasso(),                estimator_transf,matriz_rfe,rs,blog_X_train_rfe,blog_y_train)\n",
    "    matriz_rfe = estimator_cross_val_fea(k,'Elastic Net',      ElasticNet(),           estimator_transf,matriz_rfe,rs,blog_X_train_rfe,blog_y_train)\n",
    "    matriz_rfe = estimator_cross_val_fea(k,'KNN',              KNeighborsRegressor(),  estimator_transf,matriz_rfe,rs,blog_X_train_rfe,blog_y_train)\n",
    "    matriz_rfe = estimator_cross_val_fea(k,'CART',             DecisionTreeRegressor(),estimator_transf,matriz_rfe,rs,blog_X_train_rfe,blog_y_train)\n",
    "\n",
    "matriz_rfe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another alternative to reduce the number of features is \"to score input features using a model and use a filter-based feature selection method. These are called Feature Importance methods\" (BROWNLEE, 2021). The most use Feature Importance methods are Classification and Regression Trees (CART), Random Forest, Bagged Decision Trees, and Gradient Boosting.\n",
    "\n",
    "We use the Random Forest algorithm as our feature importance method. Decision tree algorithms, such as Random Forest, \"offer importance scores based on the reduction in the criterion used to select split points, like Gini or entropy\" (BROWNLEE, 2021).\n",
    "\n",
    "We use the same number of features as in the previous selection methods. Thus, we select the 190, 150, 70, and 30 most relevant features and evaluate the models again. We see that the 30 more important features represent almost 80% of all importance. With the 70 most important features we reach 90%, and with the 150 most important 99%. \n",
    "\n",
    "**We can see that that KNN was the best model and that it performed similarly for the four cases tested**. Besides, the models performed a little worse with these features than with the features selected using the RFE method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest for feature importance on a regression problem\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# feature selection\n",
    "def select_features_FI(X_train, y_train,):\n",
    "    # configure to select all features\n",
    "    RFR = RandomForestRegressor()\n",
    "    # learn relationship from training data\n",
    "    RFR.fit(X_train, y_train)\n",
    "    # transform train input data\n",
    "    importance = RFR.feature_importances_\n",
    "    return importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection\n",
    "importance = select_features_FI(blog_X_train, blog_y_train)\n",
    "\n",
    "# what are scores for the features\n",
    "FI = pd.DataFrame(importance, columns = ['Importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>features</th>\n",
       "      <th>model</th>\n",
       "      <th>MAE_mean</th>\n",
       "      <th>MAE_std</th>\n",
       "      <th>RMSE_mean</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>R2_mean</th>\n",
       "      <th>R2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.208629</td>\n",
       "      <td>0.197936</td>\n",
       "      <td>30.305936</td>\n",
       "      <td>1.129168</td>\n",
       "      <td>0.351328</td>\n",
       "      <td>0.030278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.185319</td>\n",
       "      <td>0.192488</td>\n",
       "      <td>30.311341</td>\n",
       "      <td>1.122996</td>\n",
       "      <td>0.351075</td>\n",
       "      <td>0.030518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.102784</td>\n",
       "      <td>0.170983</td>\n",
       "      <td>30.285397</td>\n",
       "      <td>1.151886</td>\n",
       "      <td>0.352246</td>\n",
       "      <td>0.030287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.122356</td>\n",
       "      <td>0.172949</td>\n",
       "      <td>30.288390</td>\n",
       "      <td>1.149228</td>\n",
       "      <td>0.352111</td>\n",
       "      <td>0.030341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.385736</td>\n",
       "      <td>0.223781</td>\n",
       "      <td>28.531051</td>\n",
       "      <td>1.311944</td>\n",
       "      <td>0.425339</td>\n",
       "      <td>0.029447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.355993</td>\n",
       "      <td>0.219612</td>\n",
       "      <td>33.743042</td>\n",
       "      <td>1.089966</td>\n",
       "      <td>0.194396</td>\n",
       "      <td>0.055561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.295442</td>\n",
       "      <td>0.184356</td>\n",
       "      <td>30.330668</td>\n",
       "      <td>1.131971</td>\n",
       "      <td>0.350254</td>\n",
       "      <td>0.030730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.288471</td>\n",
       "      <td>0.186280</td>\n",
       "      <td>30.331466</td>\n",
       "      <td>1.134145</td>\n",
       "      <td>0.350223</td>\n",
       "      <td>0.030741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.113311</td>\n",
       "      <td>0.176450</td>\n",
       "      <td>30.291149</td>\n",
       "      <td>1.156861</td>\n",
       "      <td>0.351977</td>\n",
       "      <td>0.031008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.134904</td>\n",
       "      <td>0.183125</td>\n",
       "      <td>30.301134</td>\n",
       "      <td>1.148820</td>\n",
       "      <td>0.351541</td>\n",
       "      <td>0.030901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>70</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.397965</td>\n",
       "      <td>0.218309</td>\n",
       "      <td>28.615599</td>\n",
       "      <td>1.254866</td>\n",
       "      <td>0.421949</td>\n",
       "      <td>0.027108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.461393</td>\n",
       "      <td>0.293485</td>\n",
       "      <td>33.675495</td>\n",
       "      <td>1.977965</td>\n",
       "      <td>0.199257</td>\n",
       "      <td>0.061369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>150</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.444099</td>\n",
       "      <td>0.167406</td>\n",
       "      <td>30.374028</td>\n",
       "      <td>1.117694</td>\n",
       "      <td>0.348379</td>\n",
       "      <td>0.030588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>150</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.440118</td>\n",
       "      <td>0.168038</td>\n",
       "      <td>30.371687</td>\n",
       "      <td>1.117332</td>\n",
       "      <td>0.348478</td>\n",
       "      <td>0.030623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.110817</td>\n",
       "      <td>0.185537</td>\n",
       "      <td>30.304423</td>\n",
       "      <td>1.138442</td>\n",
       "      <td>0.351389</td>\n",
       "      <td>0.030751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.133604</td>\n",
       "      <td>0.185830</td>\n",
       "      <td>30.310303</td>\n",
       "      <td>1.137118</td>\n",
       "      <td>0.351133</td>\n",
       "      <td>0.030805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>150</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.387911</td>\n",
       "      <td>0.228185</td>\n",
       "      <td>28.589800</td>\n",
       "      <td>1.317743</td>\n",
       "      <td>0.423058</td>\n",
       "      <td>0.028159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>150</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.426588</td>\n",
       "      <td>0.217897</td>\n",
       "      <td>33.181804</td>\n",
       "      <td>1.640639</td>\n",
       "      <td>0.216888</td>\n",
       "      <td>0.103949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>190</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.493089</td>\n",
       "      <td>0.167189</td>\n",
       "      <td>30.390227</td>\n",
       "      <td>1.115748</td>\n",
       "      <td>0.347675</td>\n",
       "      <td>0.030684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>190</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.490867</td>\n",
       "      <td>0.169898</td>\n",
       "      <td>30.387115</td>\n",
       "      <td>1.117990</td>\n",
       "      <td>0.347810</td>\n",
       "      <td>0.030740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>190</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.110817</td>\n",
       "      <td>0.185537</td>\n",
       "      <td>30.304423</td>\n",
       "      <td>1.138442</td>\n",
       "      <td>0.351389</td>\n",
       "      <td>0.030751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>190</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.133602</td>\n",
       "      <td>0.185821</td>\n",
       "      <td>30.310496</td>\n",
       "      <td>1.136838</td>\n",
       "      <td>0.351124</td>\n",
       "      <td>0.030802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>190</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.388277</td>\n",
       "      <td>0.227648</td>\n",
       "      <td>28.589934</td>\n",
       "      <td>1.317930</td>\n",
       "      <td>0.423053</td>\n",
       "      <td>0.028167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>190</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.406261</td>\n",
       "      <td>0.282191</td>\n",
       "      <td>33.274056</td>\n",
       "      <td>1.947994</td>\n",
       "      <td>0.211473</td>\n",
       "      <td>0.117220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   features              model  MAE_mean   MAE_std  RMSE_mean  RMSE_std  \\\n",
       "0        30  Linear Regression  9.208629  0.197936  30.305936  1.129168   \n",
       "1        30   Ridge Regression  9.185319  0.192488  30.311341  1.122996   \n",
       "2        30              Lasso  9.102784  0.170983  30.285397  1.151886   \n",
       "3        30        Elastic Net  9.122356  0.172949  30.288390  1.149228   \n",
       "4        30                KNN  6.385736  0.223781  28.531051  1.311944   \n",
       "5        30               CART  6.355993  0.219612  33.743042  1.089966   \n",
       "6        70  Linear Regression  9.295442  0.184356  30.330668  1.131971   \n",
       "7        70   Ridge Regression  9.288471  0.186280  30.331466  1.134145   \n",
       "8        70              Lasso  9.113311  0.176450  30.291149  1.156861   \n",
       "9        70        Elastic Net  9.134904  0.183125  30.301134  1.148820   \n",
       "10       70                KNN  6.397965  0.218309  28.615599  1.254866   \n",
       "11       70               CART  6.461393  0.293485  33.675495  1.977965   \n",
       "12      150  Linear Regression  9.444099  0.167406  30.374028  1.117694   \n",
       "13      150   Ridge Regression  9.440118  0.168038  30.371687  1.117332   \n",
       "14      150              Lasso  9.110817  0.185537  30.304423  1.138442   \n",
       "15      150        Elastic Net  9.133604  0.185830  30.310303  1.137118   \n",
       "16      150                KNN  6.387911  0.228185  28.589800  1.317743   \n",
       "17      150               CART  6.426588  0.217897  33.181804  1.640639   \n",
       "18      190  Linear Regression  9.493089  0.167189  30.390227  1.115748   \n",
       "19      190   Ridge Regression  9.490867  0.169898  30.387115  1.117990   \n",
       "20      190              Lasso  9.110817  0.185537  30.304423  1.138442   \n",
       "21      190        Elastic Net  9.133602  0.185821  30.310496  1.136838   \n",
       "22      190                KNN  6.388277  0.227648  28.589934  1.317930   \n",
       "23      190               CART  6.406261  0.282191  33.274056  1.947994   \n",
       "\n",
       "     R2_mean    R2_std  \n",
       "0   0.351328  0.030278  \n",
       "1   0.351075  0.030518  \n",
       "2   0.352246  0.030287  \n",
       "3   0.352111  0.030341  \n",
       "4   0.425339  0.029447  \n",
       "5   0.194396  0.055561  \n",
       "6   0.350254  0.030730  \n",
       "7   0.350223  0.030741  \n",
       "8   0.351977  0.031008  \n",
       "9   0.351541  0.030901  \n",
       "10  0.421949  0.027108  \n",
       "11  0.199257  0.061369  \n",
       "12  0.348379  0.030588  \n",
       "13  0.348478  0.030623  \n",
       "14  0.351389  0.030751  \n",
       "15  0.351133  0.030805  \n",
       "16  0.423058  0.028159  \n",
       "17  0.216888  0.103949  \n",
       "18  0.347675  0.030684  \n",
       "19  0.347810  0.030740  \n",
       "20  0.351389  0.030751  \n",
       "21  0.351124  0.030802  \n",
       "22  0.423053  0.028167  \n",
       "23  0.211473  0.117220  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matriz_fi = pd.DataFrame(columns=['features','model','MAE_mean','MAE_std','RMSE_mean','RMSE_std','R2_mean','R2_std'])\n",
    "\n",
    "for k in [30,70,150,190]:\n",
    "\n",
    "    best_features_fi = FI.transpose()\n",
    "    best_features_fi.columns = blog_X_train.columns\n",
    "    best_features_fi.sort_values('Importance',axis=1,ascending=False,inplace=True)\n",
    "    best_features_fi.drop(best_features_fi.iloc[:,k:],axis=1,inplace=True)\n",
    "    blog_X_train_fi = blog_X_train[best_features_fi.columns]\n",
    "  \n",
    "    matriz_fi = estimator_cross_val_fea(k,'Linear Regression',LinearRegression(),     estimator_transf,matriz_fi,rs,blog_X_train_fi,blog_y_train)\n",
    "    matriz_fi = estimator_cross_val_fea(k,'Ridge Regression', Ridge(),                estimator_transf,matriz_fi,rs,blog_X_train_fi,blog_y_train)\n",
    "    matriz_fi = estimator_cross_val_fea(k,'Lasso',            Lasso(),                estimator_transf,matriz_fi,rs,blog_X_train_fi,blog_y_train)\n",
    "    matriz_fi = estimator_cross_val_fea(k,'Elastic Net',      ElasticNet(),           estimator_transf,matriz_fi,rs,blog_X_train_fi,blog_y_train)\n",
    "    matriz_fi = estimator_cross_val_fea(k,'KNN',              KNeighborsRegressor(),  estimator_transf,matriz_fi,rs,blog_X_train_fi,blog_y_train)\n",
    "    matriz_fi = estimator_cross_val_fea(k,'CART',             DecisionTreeRegressor(),estimator_transf,matriz_fi,rs,blog_X_train_fi,blog_y_train)\n",
    "\n",
    "matriz_fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we compare the features selected by each model. We see that the percentage of features selected by all methods increase as the number of features used increases:\n",
    "\n",
    "1. 30 best - 9 shared (30.0%)\n",
    "2. 70 best - 32 shared (45.7%)\n",
    "3. 150 best - 92 shared (61.3%)\n",
    "4. 190 best - 134 shared (70.5%)\n",
    "\n",
    "Besides, we can argue that the 9 features that were selected by all methods as one of the 30 most relevant fatures are the most significant ones for predicting our target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_30 = MI.sort_values(by=['Score'],ascending=False)[:30].reset_index()\n",
    "FI_30 = FI.sort_values(by=['Importance'],ascending=False)[:30].reset_index()\n",
    "RF_30 = rfe_features.sort_values(by=['30 Features'],ascending=False)[:30].reset_index()\n",
    "RF_30 = RF_30.drop(columns=['70 Features','150 Features','190 Features'])\n",
    "merged_30 = pd.merge(FI_30, MI_30, on=['index'], how='inner')\n",
    "merged_30 = pd.merge(merged_30, RF_30, on=['index'], how='inner')\n",
    "len(merged_30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_70 = MI.sort_values(by=['Score'],ascending=False)[:70].reset_index()\n",
    "FI_70 = FI.sort_values(by=['Importance'],ascending=False)[:70].reset_index()\n",
    "RF_70 = rfe_features.sort_values(by=['70 Features'],ascending=False)[:70].reset_index()\n",
    "RF_70 = RF_70.drop(columns=['30 Features','150 Features','190 Features'])\n",
    "merged_70 = pd.merge(FI_70, MI_70, on=['index'], how='inner')\n",
    "merged_70 = pd.merge(merged_70, RF_70, on=['index'], how='inner')\n",
    "len(merged_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_150 = MI.sort_values(by=['Score'],ascending=False)[:150].reset_index()\n",
    "FI_150 = FI.sort_values(by=['Importance'],ascending=False)[:150].reset_index()\n",
    "RF_150 = rfe_features.sort_values(by=['150 Features'],ascending=False)[:150].reset_index()\n",
    "RF_150 = RF_150.drop(columns=['30 Features','70 Features','190 Features'])\n",
    "merged_150 = pd.merge(FI_150, MI_150, on=['index'], how='inner')\n",
    "merged_150 = pd.merge(merged_150, RF_150, on=['index'], how='inner')\n",
    "len(merged_150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "134"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MI_190 = MI.sort_values(by=['Score'],ascending=False)[:190].reset_index()\n",
    "FI_190 = FI.sort_values(by=['Importance'],ascending=False)[:190].reset_index()\n",
    "RF_190 = rfe_features.sort_values(by=['190 Features'],ascending=False)[:190].reset_index()\n",
    "RF_190 = RF_190.drop(columns=['30 Features','70 Features','150 Features'])\n",
    "merged_190 = pd.merge(FI_190, MI_190, on=['index'], how='inner')\n",
    "merged_190 = pd.merge(merged_190, RF_190, on=['index'], how='inner')\n",
    "len(merged_190)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Dimensionality reduction refers to techniques that reduce the number of input variables in a dataset. Fewer input dimensions often mean correspondingly fewer parameters or a simpler structure in the machine learning model, referred to as degrees of freedom. A model with too many degrees of freedom is likely to overfit the training dataset and therefore may not perform well on new data\" (BROWNLEE, 2021).\n",
    "\n",
    "There are several techniques to reduce a data set dimensionality. **In this notebook, we use the Principal Component Analysis (PCA), which is the most used method for dimensionality reduction**. \"It can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data\" (BROWNLEE, 2021). \n",
    "\n",
    "We reduce the dimensionality of the data set using the same number of features used in the feature selection section, 30, 70, 150, and 190. **We verify that the results are quite similar to the ones we obtained in the feature selection and that the level of the dimensionality reduction have little influence in the results**. Besides, once more the KNN model is better than the other models tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dimensionality</th>\n",
       "      <th>model</th>\n",
       "      <th>MAE_mean</th>\n",
       "      <th>MAE_std</th>\n",
       "      <th>RMSE_mean</th>\n",
       "      <th>RMSE_std</th>\n",
       "      <th>R2_mean</th>\n",
       "      <th>R2_std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.170274</td>\n",
       "      <td>0.196680</td>\n",
       "      <td>30.337866</td>\n",
       "      <td>1.132621</td>\n",
       "      <td>0.349924</td>\n",
       "      <td>0.031216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.170276</td>\n",
       "      <td>0.196676</td>\n",
       "      <td>30.337863</td>\n",
       "      <td>1.132618</td>\n",
       "      <td>0.349925</td>\n",
       "      <td>0.031216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.122299</td>\n",
       "      <td>0.180373</td>\n",
       "      <td>30.305504</td>\n",
       "      <td>1.149764</td>\n",
       "      <td>0.351367</td>\n",
       "      <td>0.030623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.145455</td>\n",
       "      <td>0.185224</td>\n",
       "      <td>30.312383</td>\n",
       "      <td>1.147534</td>\n",
       "      <td>0.351061</td>\n",
       "      <td>0.030783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.391033</td>\n",
       "      <td>0.215455</td>\n",
       "      <td>28.586793</td>\n",
       "      <td>1.320662</td>\n",
       "      <td>0.423153</td>\n",
       "      <td>0.028792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>30</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.667366</td>\n",
       "      <td>0.249153</td>\n",
       "      <td>34.565825</td>\n",
       "      <td>1.622042</td>\n",
       "      <td>0.151668</td>\n",
       "      <td>0.099029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>70</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.291957</td>\n",
       "      <td>0.198461</td>\n",
       "      <td>30.338712</td>\n",
       "      <td>1.113666</td>\n",
       "      <td>0.349879</td>\n",
       "      <td>0.030748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>70</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.292803</td>\n",
       "      <td>0.200156</td>\n",
       "      <td>30.337724</td>\n",
       "      <td>1.111993</td>\n",
       "      <td>0.349920</td>\n",
       "      <td>0.030711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>70</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.122299</td>\n",
       "      <td>0.180373</td>\n",
       "      <td>30.305504</td>\n",
       "      <td>1.149764</td>\n",
       "      <td>0.351367</td>\n",
       "      <td>0.030623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>70</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.144900</td>\n",
       "      <td>0.185320</td>\n",
       "      <td>30.311839</td>\n",
       "      <td>1.147532</td>\n",
       "      <td>0.351086</td>\n",
       "      <td>0.030759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>70</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.386980</td>\n",
       "      <td>0.224841</td>\n",
       "      <td>28.576718</td>\n",
       "      <td>1.326137</td>\n",
       "      <td>0.423587</td>\n",
       "      <td>0.028486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>70</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.879595</td>\n",
       "      <td>0.118472</td>\n",
       "      <td>34.956786</td>\n",
       "      <td>0.753124</td>\n",
       "      <td>0.132987</td>\n",
       "      <td>0.084415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>150</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.459503</td>\n",
       "      <td>0.168487</td>\n",
       "      <td>30.358749</td>\n",
       "      <td>1.116275</td>\n",
       "      <td>0.349027</td>\n",
       "      <td>0.030679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>150</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.458508</td>\n",
       "      <td>0.169914</td>\n",
       "      <td>30.359304</td>\n",
       "      <td>1.116341</td>\n",
       "      <td>0.349005</td>\n",
       "      <td>0.030642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>150</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.122299</td>\n",
       "      <td>0.180373</td>\n",
       "      <td>30.305504</td>\n",
       "      <td>1.149764</td>\n",
       "      <td>0.351367</td>\n",
       "      <td>0.030623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>150</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.144900</td>\n",
       "      <td>0.185320</td>\n",
       "      <td>30.311839</td>\n",
       "      <td>1.147532</td>\n",
       "      <td>0.351086</td>\n",
       "      <td>0.030759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>150</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.392342</td>\n",
       "      <td>0.226349</td>\n",
       "      <td>28.583801</td>\n",
       "      <td>1.326865</td>\n",
       "      <td>0.423308</td>\n",
       "      <td>0.028381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>150</td>\n",
       "      <td>CART</td>\n",
       "      <td>6.962126</td>\n",
       "      <td>0.287425</td>\n",
       "      <td>35.071667</td>\n",
       "      <td>1.803805</td>\n",
       "      <td>0.132001</td>\n",
       "      <td>0.048294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>190</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>9.504434</td>\n",
       "      <td>0.161015</td>\n",
       "      <td>30.368124</td>\n",
       "      <td>1.113705</td>\n",
       "      <td>0.348606</td>\n",
       "      <td>0.031003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>190</td>\n",
       "      <td>Ridge Regression</td>\n",
       "      <td>9.504412</td>\n",
       "      <td>0.160918</td>\n",
       "      <td>30.368032</td>\n",
       "      <td>1.113380</td>\n",
       "      <td>0.348609</td>\n",
       "      <td>0.031009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>190</td>\n",
       "      <td>Lasso</td>\n",
       "      <td>9.122299</td>\n",
       "      <td>0.180373</td>\n",
       "      <td>30.305504</td>\n",
       "      <td>1.149764</td>\n",
       "      <td>0.351367</td>\n",
       "      <td>0.030623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>190</td>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>9.144900</td>\n",
       "      <td>0.185320</td>\n",
       "      <td>30.311839</td>\n",
       "      <td>1.147532</td>\n",
       "      <td>0.351086</td>\n",
       "      <td>0.030759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>190</td>\n",
       "      <td>KNN</td>\n",
       "      <td>6.390472</td>\n",
       "      <td>0.224537</td>\n",
       "      <td>28.581474</td>\n",
       "      <td>1.325347</td>\n",
       "      <td>0.423397</td>\n",
       "      <td>0.028422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>190</td>\n",
       "      <td>CART</td>\n",
       "      <td>7.055118</td>\n",
       "      <td>0.209190</td>\n",
       "      <td>35.846981</td>\n",
       "      <td>1.809665</td>\n",
       "      <td>0.091019</td>\n",
       "      <td>0.082416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dimensionality              model  MAE_mean   MAE_std  RMSE_mean  RMSE_std  \\\n",
       "0              30  Linear Regression  9.170274  0.196680  30.337866  1.132621   \n",
       "1              30   Ridge Regression  9.170276  0.196676  30.337863  1.132618   \n",
       "2              30              Lasso  9.122299  0.180373  30.305504  1.149764   \n",
       "3              30        Elastic Net  9.145455  0.185224  30.312383  1.147534   \n",
       "4              30                KNN  6.391033  0.215455  28.586793  1.320662   \n",
       "5              30               CART  6.667366  0.249153  34.565825  1.622042   \n",
       "6              70  Linear Regression  9.291957  0.198461  30.338712  1.113666   \n",
       "7              70   Ridge Regression  9.292803  0.200156  30.337724  1.111993   \n",
       "8              70              Lasso  9.122299  0.180373  30.305504  1.149764   \n",
       "9              70        Elastic Net  9.144900  0.185320  30.311839  1.147532   \n",
       "10             70                KNN  6.386980  0.224841  28.576718  1.326137   \n",
       "11             70               CART  6.879595  0.118472  34.956786  0.753124   \n",
       "12            150  Linear Regression  9.459503  0.168487  30.358749  1.116275   \n",
       "13            150   Ridge Regression  9.458508  0.169914  30.359304  1.116341   \n",
       "14            150              Lasso  9.122299  0.180373  30.305504  1.149764   \n",
       "15            150        Elastic Net  9.144900  0.185320  30.311839  1.147532   \n",
       "16            150                KNN  6.392342  0.226349  28.583801  1.326865   \n",
       "17            150               CART  6.962126  0.287425  35.071667  1.803805   \n",
       "18            190  Linear Regression  9.504434  0.161015  30.368124  1.113705   \n",
       "19            190   Ridge Regression  9.504412  0.160918  30.368032  1.113380   \n",
       "20            190              Lasso  9.122299  0.180373  30.305504  1.149764   \n",
       "21            190        Elastic Net  9.144900  0.185320  30.311839  1.147532   \n",
       "22            190                KNN  6.390472  0.224537  28.581474  1.325347   \n",
       "23            190               CART  7.055118  0.209190  35.846981  1.809665   \n",
       "\n",
       "     R2_mean    R2_std  \n",
       "0   0.349924  0.031216  \n",
       "1   0.349925  0.031216  \n",
       "2   0.351367  0.030623  \n",
       "3   0.351061  0.030783  \n",
       "4   0.423153  0.028792  \n",
       "5   0.151668  0.099029  \n",
       "6   0.349879  0.030748  \n",
       "7   0.349920  0.030711  \n",
       "8   0.351367  0.030623  \n",
       "9   0.351086  0.030759  \n",
       "10  0.423587  0.028486  \n",
       "11  0.132987  0.084415  \n",
       "12  0.349027  0.030679  \n",
       "13  0.349005  0.030642  \n",
       "14  0.351367  0.030623  \n",
       "15  0.351086  0.030759  \n",
       "16  0.423308  0.028381  \n",
       "17  0.132001  0.048294  \n",
       "18  0.348606  0.031003  \n",
       "19  0.348609  0.031009  \n",
       "20  0.351367  0.030623  \n",
       "21  0.351086  0.030759  \n",
       "22  0.423397  0.028422  \n",
       "23  0.091019  0.082416  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def estimator_pca(estimator,k):\n",
    "    #imputer = SimpleImputer(strategy='median')\n",
    "    pipeline = Pipeline(steps=[('pca',PCA(n_components=k)),('model', estimator)])\n",
    "    return pipeline \n",
    "\n",
    "def estimator_cross_val_pca(k,model,estimator,pipe,matriz,rs,X,y):\n",
    "    pipe_ = pipe(estimator,k)\n",
    "    scoring = ['neg_mean_absolute_error', 'neg_root_mean_squared_error','r2']\n",
    "    kfold = KFold(n_splits=5, random_state=rs,shuffle=True)\n",
    "    scores = cross_validate(pipe_,X,y,cv=kfold,scoring=scoring)\n",
    "    \n",
    "    mae_scores = -scores.get('test_neg_mean_absolute_error')\n",
    "    mae_mean = mae_scores.mean()\n",
    "    mae_std = mae_scores.std()\n",
    "    \n",
    "    rmse_scores = -scores.get('test_neg_root_mean_squared_error')\n",
    "    rmse_mean = rmse_scores.mean()\n",
    "    rmse_std = rmse_scores.std()\n",
    "    \n",
    "    r2_scores = scores.get('test_r2')\n",
    "    r2_mean = r2_scores.mean()\n",
    "    r2_std = r2_scores.std()\n",
    "    \n",
    "    results_ = [k,model,mae_mean,mae_std,rmse_mean,rmse_std,r2_mean,r2_std]\n",
    "    results_ = pd.Series(results_, index = matriz.columns)\n",
    "    results = matriz.append(results_,ignore_index=True)\n",
    "    return results\n",
    "\n",
    "matriz_pca = pd.DataFrame(columns=['dimensionality','model','MAE_mean','MAE_std','RMSE_mean','RMSE_std','R2_mean','R2_std'])\n",
    "\n",
    "for k in [30,70,150,190]:\n",
    "  \n",
    "    matriz_pca = estimator_cross_val_pca(k,'Linear Regression',LinearRegression(),     estimator_pca,matriz_pca,rs,blog_X_train,blog_y_train)\n",
    "    matriz_pca = estimator_cross_val_pca(k,'Ridge Regression', Ridge(),                estimator_pca,matriz_pca,rs,blog_X_train,blog_y_train)\n",
    "    matriz_pca = estimator_cross_val_pca(k,'Lasso',            Lasso(),                estimator_pca,matriz_pca,rs,blog_X_train,blog_y_train)\n",
    "    matriz_pca = estimator_cross_val_pca(k,'Elastic Net',      ElasticNet(),           estimator_pca,matriz_pca,rs,blog_X_train,blog_y_train)\n",
    "    matriz_pca = estimator_cross_val_pca(k,'KNN',              KNeighborsRegressor(),  estimator_pca,matriz_pca,rs,blog_X_train,blog_y_train)\n",
    "    matriz_pca = estimator_cross_val_pca(k,'CART',             DecisionTreeRegressor(),estimator_pca,matriz_pca,rs,blog_X_train,blog_y_train)\n",
    "\n",
    "matriz_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluate the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now evaluate the performance of our ML model in the test set, to see how it perform with unseen data.\n",
    "\n",
    "We will do two tests. In the first one we use the KNN model and the 30 features selected using the RFE method. For the second test, we use the KNN model and the data set reduced using the PCA method.\n",
    "\n",
    "First, we import the test set.\n",
    "\n",
    "After testing the models, we verify that the performance of our model with the test set is similar to the performance with the train set. The MAE and RMSE are actually a little better but the R² is lower. Besides, the RMSE is considrably higher than the MAE. This result suggests that our data has many outliers and, consequently, our model is making some big errors.\n",
    "\n",
    "Finally, we see that using the features selected by the RFE method and doing a dimensionality reduction using the PCA have similiar results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "os.chdir(r\"/Users/leuzinger/Dropbox/Data Science/Awari/Regressions/BlogFeedback/Test/\")\n",
    "filenames = [i for i in glob.glob(\"*.csv\")]\n",
    "df = [pd.read_csv(file, sep = \",\", header=None,) \n",
    "      for file in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>blog_avg_total</th>\n",
       "      <th>blog_std_total</th>\n",
       "      <th>blog_min_total</th>\n",
       "      <th>blog_max_total</th>\n",
       "      <th>blog_median_total</th>\n",
       "      <th>blog_avg_last24h</th>\n",
       "      <th>blog_std_last24h</th>\n",
       "      <th>blog_min_last24h</th>\n",
       "      <th>blog_max_last24h</th>\n",
       "      <th>blog_median_last24h</th>\n",
       "      <th>blog_avg_24-48h</th>\n",
       "      <th>blog_std_24-48h</th>\n",
       "      <th>blog_min_24-48h</th>\n",
       "      <th>blog_max_24-48h</th>\n",
       "      <th>blog_median_24-48h</th>\n",
       "      <th>blog_avg_first24h</th>\n",
       "      <th>blog_std_first24h</th>\n",
       "      <th>blog_min_first24h</th>\n",
       "      <th>blog_max_first24h</th>\n",
       "      <th>blog_median_first24h</th>\n",
       "      <th>blog_avg_difference</th>\n",
       "      <th>blog_std_difference</th>\n",
       "      <th>blog_min_difference</th>\n",
       "      <th>blog_max_difference</th>\n",
       "      <th>blog_median_difference</th>\n",
       "      <th>blog_avg_total_tr</th>\n",
       "      <th>blog_std_total_tr</th>\n",
       "      <th>blog_min_total_tr</th>\n",
       "      <th>blog_max_total_tr</th>\n",
       "      <th>blog_median_total_tr</th>\n",
       "      <th>blog_avg_last24h_tr</th>\n",
       "      <th>blog_std_last24h_tr</th>\n",
       "      <th>blog_min_last24h_tr</th>\n",
       "      <th>blog_max_last24h_tr</th>\n",
       "      <th>blog_median_last24h_tr</th>\n",
       "      <th>blog_avg_24-48h_tr</th>\n",
       "      <th>blog_std_24-48h_tr</th>\n",
       "      <th>blog_min_24-48h_tr</th>\n",
       "      <th>blog_max_24-48h_tr</th>\n",
       "      <th>blog_median_24-48h_tr</th>\n",
       "      <th>blog_avg_first24h_tr</th>\n",
       "      <th>blog_std_first24h_tr</th>\n",
       "      <th>blog_min_first24h_tr</th>\n",
       "      <th>blog_max_first24h_tr</th>\n",
       "      <th>blog_median_first24h_tr</th>\n",
       "      <th>blog_avg_difference_tr</th>\n",
       "      <th>blog_std_difference_tr</th>\n",
       "      <th>blog_min_difference_tr</th>\n",
       "      <th>blog_max_difference_tr</th>\n",
       "      <th>blog_median_difference_tr</th>\n",
       "      <th>total</th>\n",
       "      <th>last24h</th>\n",
       "      <th>24-48h</th>\n",
       "      <th>first24h</th>\n",
       "      <th>difference</th>\n",
       "      <th>total_tr</th>\n",
       "      <th>last24h_tr</th>\n",
       "      <th>24-48h_tr</th>\n",
       "      <th>first24h_tr</th>\n",
       "      <th>difference_tr</th>\n",
       "      <th>time_first_post</th>\n",
       "      <th>lenght_post</th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>word3</th>\n",
       "      <th>word4</th>\n",
       "      <th>word5</th>\n",
       "      <th>word6</th>\n",
       "      <th>word7</th>\n",
       "      <th>word8</th>\n",
       "      <th>word9</th>\n",
       "      <th>word10</th>\n",
       "      <th>word11</th>\n",
       "      <th>word12</th>\n",
       "      <th>word13</th>\n",
       "      <th>word14</th>\n",
       "      <th>word15</th>\n",
       "      <th>word16</th>\n",
       "      <th>word17</th>\n",
       "      <th>word18</th>\n",
       "      <th>word19</th>\n",
       "      <th>word20</th>\n",
       "      <th>word21</th>\n",
       "      <th>word22</th>\n",
       "      <th>word23</th>\n",
       "      <th>word24</th>\n",
       "      <th>word25</th>\n",
       "      <th>word26</th>\n",
       "      <th>word27</th>\n",
       "      <th>word28</th>\n",
       "      <th>word29</th>\n",
       "      <th>word30</th>\n",
       "      <th>word31</th>\n",
       "      <th>word32</th>\n",
       "      <th>word33</th>\n",
       "      <th>word34</th>\n",
       "      <th>word35</th>\n",
       "      <th>word36</th>\n",
       "      <th>word37</th>\n",
       "      <th>word38</th>\n",
       "      <th>word39</th>\n",
       "      <th>word40</th>\n",
       "      <th>word41</th>\n",
       "      <th>word42</th>\n",
       "      <th>word43</th>\n",
       "      <th>word44</th>\n",
       "      <th>word45</th>\n",
       "      <th>word46</th>\n",
       "      <th>word47</th>\n",
       "      <th>word48</th>\n",
       "      <th>word49</th>\n",
       "      <th>word50</th>\n",
       "      <th>word51</th>\n",
       "      <th>word52</th>\n",
       "      <th>word53</th>\n",
       "      <th>word54</th>\n",
       "      <th>word55</th>\n",
       "      <th>word56</th>\n",
       "      <th>word57</th>\n",
       "      <th>word58</th>\n",
       "      <th>word59</th>\n",
       "      <th>word60</th>\n",
       "      <th>word61</th>\n",
       "      <th>word62</th>\n",
       "      <th>word63</th>\n",
       "      <th>word64</th>\n",
       "      <th>word65</th>\n",
       "      <th>word66</th>\n",
       "      <th>word67</th>\n",
       "      <th>word68</th>\n",
       "      <th>word69</th>\n",
       "      <th>word70</th>\n",
       "      <th>word71</th>\n",
       "      <th>word72</th>\n",
       "      <th>word73</th>\n",
       "      <th>word74</th>\n",
       "      <th>word75</th>\n",
       "      <th>word76</th>\n",
       "      <th>word77</th>\n",
       "      <th>word78</th>\n",
       "      <th>word79</th>\n",
       "      <th>word80</th>\n",
       "      <th>word81</th>\n",
       "      <th>word82</th>\n",
       "      <th>word83</th>\n",
       "      <th>word84</th>\n",
       "      <th>word85</th>\n",
       "      <th>word86</th>\n",
       "      <th>word87</th>\n",
       "      <th>word88</th>\n",
       "      <th>word89</th>\n",
       "      <th>word90</th>\n",
       "      <th>word91</th>\n",
       "      <th>word92</th>\n",
       "      <th>word93</th>\n",
       "      <th>word94</th>\n",
       "      <th>word95</th>\n",
       "      <th>word96</th>\n",
       "      <th>word97</th>\n",
       "      <th>word98</th>\n",
       "      <th>word99</th>\n",
       "      <th>word100</th>\n",
       "      <th>word101</th>\n",
       "      <th>word102</th>\n",
       "      <th>word103</th>\n",
       "      <th>word104</th>\n",
       "      <th>word105</th>\n",
       "      <th>word106</th>\n",
       "      <th>word107</th>\n",
       "      <th>word108</th>\n",
       "      <th>word109</th>\n",
       "      <th>word110</th>\n",
       "      <th>word111</th>\n",
       "      <th>word112</th>\n",
       "      <th>word113</th>\n",
       "      <th>word114</th>\n",
       "      <th>word115</th>\n",
       "      <th>word116</th>\n",
       "      <th>word117</th>\n",
       "      <th>word118</th>\n",
       "      <th>word119</th>\n",
       "      <th>word120</th>\n",
       "      <th>word121</th>\n",
       "      <th>word122</th>\n",
       "      <th>word123</th>\n",
       "      <th>word124</th>\n",
       "      <th>word125</th>\n",
       "      <th>word126</th>\n",
       "      <th>word127</th>\n",
       "      <th>word128</th>\n",
       "      <th>word129</th>\n",
       "      <th>word130</th>\n",
       "      <th>word131</th>\n",
       "      <th>word132</th>\n",
       "      <th>word133</th>\n",
       "      <th>word134</th>\n",
       "      <th>word135</th>\n",
       "      <th>word136</th>\n",
       "      <th>word137</th>\n",
       "      <th>word138</th>\n",
       "      <th>word139</th>\n",
       "      <th>word140</th>\n",
       "      <th>word141</th>\n",
       "      <th>word142</th>\n",
       "      <th>word143</th>\n",
       "      <th>word144</th>\n",
       "      <th>word145</th>\n",
       "      <th>word146</th>\n",
       "      <th>word147</th>\n",
       "      <th>word148</th>\n",
       "      <th>word149</th>\n",
       "      <th>word150</th>\n",
       "      <th>word151</th>\n",
       "      <th>word152</th>\n",
       "      <th>word153</th>\n",
       "      <th>word154</th>\n",
       "      <th>word155</th>\n",
       "      <th>word156</th>\n",
       "      <th>word157</th>\n",
       "      <th>word158</th>\n",
       "      <th>word159</th>\n",
       "      <th>word160</th>\n",
       "      <th>word161</th>\n",
       "      <th>word162</th>\n",
       "      <th>word163</th>\n",
       "      <th>word164</th>\n",
       "      <th>word165</th>\n",
       "      <th>word166</th>\n",
       "      <th>word167</th>\n",
       "      <th>word168</th>\n",
       "      <th>word169</th>\n",
       "      <th>word170</th>\n",
       "      <th>word171</th>\n",
       "      <th>word172</th>\n",
       "      <th>word173</th>\n",
       "      <th>word174</th>\n",
       "      <th>word175</th>\n",
       "      <th>word176</th>\n",
       "      <th>word177</th>\n",
       "      <th>word178</th>\n",
       "      <th>word179</th>\n",
       "      <th>word180</th>\n",
       "      <th>word181</th>\n",
       "      <th>word182</th>\n",
       "      <th>word183</th>\n",
       "      <th>word184</th>\n",
       "      <th>word185</th>\n",
       "      <th>word186</th>\n",
       "      <th>word187</th>\n",
       "      <th>word188</th>\n",
       "      <th>word189</th>\n",
       "      <th>word190</th>\n",
       "      <th>word191</th>\n",
       "      <th>word192</th>\n",
       "      <th>word193</th>\n",
       "      <th>word194</th>\n",
       "      <th>word195</th>\n",
       "      <th>word196</th>\n",
       "      <th>word197</th>\n",
       "      <th>word198</th>\n",
       "      <th>word199</th>\n",
       "      <th>word200</th>\n",
       "      <th>Mon_bl</th>\n",
       "      <th>Tue_bl</th>\n",
       "      <th>Wed_bl</th>\n",
       "      <th>Thu_bl</th>\n",
       "      <th>Fri_bl</th>\n",
       "      <th>Sat_bl</th>\n",
       "      <th>Sun_bl</th>\n",
       "      <th>Mon_post</th>\n",
       "      <th>Tue_post</th>\n",
       "      <th>Wed_post</th>\n",
       "      <th>Thu_post</th>\n",
       "      <th>Fri_post</th>\n",
       "      <th>Sat_post</th>\n",
       "      <th>Sun_post</th>\n",
       "      <th>parent_pages</th>\n",
       "      <th>min_parent</th>\n",
       "      <th>max_parent</th>\n",
       "      <th>avg_parent</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.24567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.176685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.176685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>1470.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>3520.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.056075</td>\n",
       "      <td>0.330159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018692</td>\n",
       "      <td>0.192442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018692</td>\n",
       "      <td>0.192442</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056075</td>\n",
       "      <td>0.330159</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.273434</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.24567</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.176685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>0.176685</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.254</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1468.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47.776787</td>\n",
       "      <td>93.737470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>17.857143</td>\n",
       "      <td>56.888218</td>\n",
       "      <td>0.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.350447</td>\n",
       "      <td>56.911470</td>\n",
       "      <td>0.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>46.386160</td>\n",
       "      <td>91.284140</td>\n",
       "      <td>1.0</td>\n",
       "      <td>595.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.506696</td>\n",
       "      <td>79.062050</td>\n",
       "      <td>-590.0</td>\n",
       "      <td>594.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   blog_avg_total  blog_std_total  blog_min_total  blog_max_total  \\\n",
       "0        0.000000        0.000000             0.0             0.0   \n",
       "1        0.000000        0.000000             0.0             0.0   \n",
       "2        0.056075        0.330159             0.0             2.0   \n",
       "3        0.000000        0.000000             0.0             0.0   \n",
       "4       47.776787       93.737470             1.0           598.0   \n",
       "\n",
       "   blog_median_total  blog_avg_last24h  blog_std_last24h  blog_min_last24h  \\\n",
       "0                0.0          0.000000          0.000000               0.0   \n",
       "1                0.0          0.000000          0.000000               0.0   \n",
       "2                0.0          0.018692          0.192442               0.0   \n",
       "3                0.0          0.000000          0.000000               0.0   \n",
       "4                7.5         17.857143         56.888218               0.0   \n",
       "\n",
       "   blog_max_last24h  blog_median_last24h  blog_avg_24-48h  blog_std_24-48h  \\\n",
       "0               0.0                  0.0         0.000000         0.000000   \n",
       "1               0.0                  0.0         0.000000         0.000000   \n",
       "2               2.0                  0.0         0.018692         0.192442   \n",
       "3               0.0                  0.0         0.000000         0.000000   \n",
       "4             594.0                  1.0        17.350447        56.911470   \n",
       "\n",
       "   blog_min_24-48h  blog_max_24-48h  blog_median_24-48h  blog_avg_first24h  \\\n",
       "0              0.0              0.0                 0.0           0.000000   \n",
       "1              0.0              0.0                 0.0           0.000000   \n",
       "2              0.0              2.0                 0.0           0.056075   \n",
       "3              0.0              0.0                 0.0           0.000000   \n",
       "4              0.0            594.0                 1.0          46.386160   \n",
       "\n",
       "   blog_std_first24h  blog_min_first24h  blog_max_first24h  \\\n",
       "0           0.000000                0.0                0.0   \n",
       "1           0.000000                0.0                0.0   \n",
       "2           0.330159                0.0                2.0   \n",
       "3           0.000000                0.0                0.0   \n",
       "4          91.284140                1.0              595.0   \n",
       "\n",
       "   blog_median_first24h  blog_avg_difference  blog_std_difference  \\\n",
       "0                   0.0             0.000000             0.000000   \n",
       "1                   0.0             0.000000             0.000000   \n",
       "2                   0.0             0.000000             0.273434   \n",
       "3                   0.0             0.000000             0.000000   \n",
       "4                   7.0             0.506696            79.062050   \n",
       "\n",
       "   blog_min_difference  blog_max_difference  blog_median_difference  \\\n",
       "0                  0.0                  0.0                     0.0   \n",
       "1                  0.0                  0.0                     0.0   \n",
       "2                 -2.0                  2.0                     0.0   \n",
       "3                  0.0                  0.0                     0.0   \n",
       "4               -590.0                594.0                     0.0   \n",
       "\n",
       "   blog_avg_total_tr  blog_std_total_tr  blog_min_total_tr  blog_max_total_tr  \\\n",
       "0           0.064516            0.24567                0.0                1.0   \n",
       "1           0.000000            0.00000                0.0                0.0   \n",
       "2           0.000000            0.00000                0.0                0.0   \n",
       "3           0.064516            0.24567                0.0                1.0   \n",
       "4           0.000000            0.00000                0.0                0.0   \n",
       "\n",
       "   blog_median_total_tr  blog_avg_last24h_tr  blog_std_last24h_tr  \\\n",
       "0                   0.0             0.032258             0.176685   \n",
       "1                   0.0             0.000000             0.000000   \n",
       "2                   0.0             0.000000             0.000000   \n",
       "3                   0.0             0.032258             0.176685   \n",
       "4                   0.0             0.000000             0.000000   \n",
       "\n",
       "   blog_min_last24h_tr  blog_max_last24h_tr  blog_median_last24h_tr  \\\n",
       "0                  0.0                  1.0                     0.0   \n",
       "1                  0.0                  0.0                     0.0   \n",
       "2                  0.0                  0.0                     0.0   \n",
       "3                  0.0                  1.0                     0.0   \n",
       "4                  0.0                  0.0                     0.0   \n",
       "\n",
       "   blog_avg_24-48h_tr  blog_std_24-48h_tr  blog_min_24-48h_tr  \\\n",
       "0            0.032258            0.176685                 0.0   \n",
       "1            0.000000            0.000000                 0.0   \n",
       "2            0.000000            0.000000                 0.0   \n",
       "3            0.032258            0.176685                 0.0   \n",
       "4            0.000000            0.000000                 0.0   \n",
       "\n",
       "   blog_max_24-48h_tr  blog_median_24-48h_tr  blog_avg_first24h_tr  \\\n",
       "0                 1.0                    0.0                   0.0   \n",
       "1                 0.0                    0.0                   0.0   \n",
       "2                 0.0                    0.0                   0.0   \n",
       "3                 1.0                    0.0                   0.0   \n",
       "4                 0.0                    0.0                   0.0   \n",
       "\n",
       "   blog_std_first24h_tr  blog_min_first24h_tr  blog_max_first24h_tr  \\\n",
       "0                   0.0                   0.0                   0.0   \n",
       "1                   0.0                   0.0                   0.0   \n",
       "2                   0.0                   0.0                   0.0   \n",
       "3                   0.0                   0.0                   0.0   \n",
       "4                   0.0                   0.0                   0.0   \n",
       "\n",
       "   blog_median_first24h_tr  blog_avg_difference_tr  blog_std_difference_tr  \\\n",
       "0                      0.0                     0.0                   0.254   \n",
       "1                      0.0                     0.0                   0.000   \n",
       "2                      0.0                     0.0                   0.000   \n",
       "3                      0.0                     0.0                   0.254   \n",
       "4                      0.0                     0.0                   0.000   \n",
       "\n",
       "   blog_min_difference_tr  blog_max_difference_tr  blog_median_difference_tr  \\\n",
       "0                    -1.0                     1.0                        0.0   \n",
       "1                     0.0                     0.0                        0.0   \n",
       "2                     0.0                     0.0                        0.0   \n",
       "3                    -1.0                     1.0                        0.0   \n",
       "4                     0.0                     0.0                        0.0   \n",
       "\n",
       "   total  last24h  24-48h  first24h  difference  total_tr  last24h_tr  \\\n",
       "0    0.0      0.0     0.0       0.0         0.0       0.0         0.0   \n",
       "1  102.0     91.0    11.0     101.0        80.0       2.0         2.0   \n",
       "2    0.0      0.0     0.0       0.0         0.0       0.0         0.0   \n",
       "3    0.0      0.0     0.0       0.0         0.0       0.0         0.0   \n",
       "4    5.0      5.0     0.0       5.0         5.0       0.0         0.0   \n",
       "\n",
       "   24-48h_tr  first24h_tr  difference_tr  time_first_post  lenght_post  word1  \\\n",
       "0        0.0          0.0            0.0             50.0       1470.0    0.0   \n",
       "1        0.0          2.0            2.0             27.0       3520.0    0.0   \n",
       "2        0.0          0.0            0.0             16.0        800.0    0.0   \n",
       "3        0.0          0.0            0.0             51.0       1468.0    0.0   \n",
       "4        0.0          0.0            0.0             14.0          0.0    0.0   \n",
       "\n",
       "   word2  word3  word4  word5  word6  word7  word8  word9  word10  word11  \\\n",
       "0    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0     0.0   \n",
       "1    1.0    0.0    0.0    1.0    0.0    0.0    0.0    0.0     0.0     0.0   \n",
       "2    1.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0     0.0   \n",
       "3    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0     0.0   \n",
       "4    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.0     0.0     0.0   \n",
       "\n",
       "   word12  word13  word14  word15  word16  word17  word18  word19  word20  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word21  word22  word23  word24  word25  word26  word27  word28  word29  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word30  word31  word32  word33  word34  word35  word36  word37  word38  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word39  word40  word41  word42  word43  word44  word45  word46  word47  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     1.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word48  word49  word50  word51  word52  word53  word54  word55  word56  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word57  word58  word59  word60  word61  word62  word63  word64  word65  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     1.0     0.0     1.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word66  word67  word68  word69  word70  word71  word72  word73  word74  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word75  word76  word77  word78  word79  word80  word81  word82  word83  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     1.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     1.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word84  word85  word86  word87  word88  word89  word90  word91  word92  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "1     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   word93  word94  word95  word96  word97  word98  word99  word100  word101  \\\n",
       "0     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0      0.0   \n",
       "1     0.0     0.0     0.0     1.0     1.0     0.0     0.0      0.0      0.0   \n",
       "2     0.0     0.0     0.0     1.0     0.0     0.0     0.0      0.0      0.0   \n",
       "3     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0      0.0   \n",
       "4     0.0     0.0     0.0     0.0     0.0     0.0     0.0      0.0      0.0   \n",
       "\n",
       "   word102  word103  word104  word105  word106  word107  word108  word109  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word110  word111  word112  word113  word114  word115  word116  word117  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word118  word119  word120  word121  word122  word123  word124  word125  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word126  word127  word128  word129  word130  word131  word132  word133  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      1.0      0.0      1.0      1.0      0.0   \n",
       "2      0.0      0.0      0.0      1.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word134  word135  word136  word137  word138  word139  word140  word141  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word142  word143  word144  word145  word146  word147  word148  word149  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word150  word151  word152  word153  word154  word155  word156  word157  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      1.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word158  word159  word160  word161  word162  word163  word164  word165  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      1.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word166  word167  word168  word169  word170  word171  word172  word173  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word174  word175  word176  word177  word178  word179  word180  word181  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word182  word183  word184  word185  word186  word187  word188  word189  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      1.0      0.0      1.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      1.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word190  word191  word192  word193  word194  word195  word196  word197  \\\n",
       "0      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "3      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "4      0.0      0.0      0.0      0.0      0.0      0.0      0.0      0.0   \n",
       "\n",
       "   word198  word199  word200  Mon_bl  Tue_bl  Wed_bl  Thu_bl  Fri_bl  Sat_bl  \\\n",
       "0      0.0      0.0      0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "1      0.0      0.0      0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "2      0.0      0.0      0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "3      0.0      0.0      0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "4      0.0      0.0      0.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "   Sun_bl  Mon_post  Tue_post  Wed_post  Thu_post  Fri_post  Sat_post  \\\n",
       "0     0.0       0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "1     0.0       0.0       0.0       0.0       0.0       0.0       0.0   \n",
       "2     0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "3     0.0       0.0       0.0       0.0       0.0       0.0       1.0   \n",
       "4     0.0       1.0       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   Sun_post  parent_pages  min_parent  max_parent  avg_parent  target  \n",
       "0       0.0           0.0         0.0         0.0         0.0     0.0  \n",
       "1       1.0           0.0         0.0         0.0         0.0     0.0  \n",
       "2       0.0           0.0         0.0         0.0         0.0     0.0  \n",
       "3       0.0           0.0         0.0         0.0         0.0     0.0  \n",
       "4       0.0           0.0         0.0         0.0         0.0     0.0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog_test = df[0]\n",
    "\n",
    "for i in range(1,len(df)):\n",
    "    blog_test = blog_test.append(df[i]) \n",
    "\n",
    "blog_test.reset_index(drop=True,inplace=True)\n",
    "blog_test.set_axis(att,axis=1,inplace=True)\n",
    "blog_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7624 entries, 0 to 7623\n",
      "Columns: 281 entries, blog_avg_total to target\n",
      "dtypes: float64(281)\n",
      "memory usage: 16.3 MB\n"
     ]
    }
   ],
   "source": [
    "blog_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_X_test = blog_test.drop('target',axis=1).copy()\n",
    "blog_y_test = blog_test['target'].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  5.79 \n",
      "RMSE: 25.12 \n",
      "R2:   0.32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=30) \n",
    "knn = KNeighborsRegressor()\n",
    "pipe_rfe = Pipeline(steps=[('rfe',rfe),('knn',knn)])\n",
    "\n",
    "pipe_rfe.fit(blog_X_train,blog_y_train)\n",
    "blog_y_hat = pipe_rfe.predict(blog_X_test)\n",
    "\n",
    "final_mae = mean_absolute_error(blog_y_test,blog_y_hat)\n",
    "final_mse = mean_squared_error(blog_y_test,blog_y_hat)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_r2 = r2_score(blog_y_test,blog_y_hat)\n",
    "print('MAE:  %.2f'%final_mae,'\\nRMSE: %.2f'%final_rmse,'\\nR2:   %.2f'%final_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  5.72 \n",
      "RMSE: 25.10 \n",
      "R2:   0.32\n"
     ]
    }
   ],
   "source": [
    "pipe_pca = Pipeline(steps=[('pca',PCA(n_components=30)),('knn', KNeighborsRegressor())])\n",
    "pipe_pca.fit(blog_X_train,blog_y_train)\n",
    "blog_y_hat = pipe_pca.predict(blog_X_test)\n",
    "\n",
    "final_mae = mean_absolute_error(blog_y_test,blog_y_hat)\n",
    "final_mse = mean_squared_error(blog_y_test,blog_y_hat)\n",
    "final_rmse = np.sqrt(final_mse)\n",
    "final_r2 = r2_score(blog_y_test,blog_y_hat)\n",
    "print('MAE:  %.2f'%final_mae,'\\nRMSE: %.2f'%final_rmse,'\\nR2:   %.2f'%final_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we created a model to predict the number of blog posts in the next 24h based on several attributes of the post. First, we tested some regression models: \n",
    "\n",
    "1. Linear regression\n",
    "2. Ridge regression\n",
    "3. Lasso regression\n",
    "4. Elastic Net\n",
    "5. Classification and Regression Trees (CART)\n",
    "6. k-Nearest Neighbors (KNN)\n",
    "\n",
    "In this first tests, the KNN was the best performing method.\n",
    "\n",
    "However, we verified that the large number of features in our data was demanding a high computing time to run the models. Therefore, we tested some techniques to reduce the number of features:\n",
    "\n",
    "1. Mutual Information Statistics\n",
    "2. Recursive Feature Elimination (RFE)\n",
    "3. Random Forest\n",
    "\n",
    "The features selected by the RFE were the ones that resulted in the best performance of the KNN model.\n",
    "\n",
    "Finaly, we also used a dimensionality reduction method, the Principal Component Analysis (PCA) to reduce the size of our data set. Our results with the train set showed that both the RFE and the PCA, combined with the KNN model, had similar results.\n",
    "\n",
    "**Therefore, we tested two models with our test set: (i) KNN + RFE and (ii) KNN + PCA. We verified that the models performed almost identically**.\n",
    "\n",
    "**However, our models performed modestly at best. All evaluation metrics used are poor**, especially the RMSE and the R². The fact that the RMSE is high suggests that our data has many outliers and, consequently, our model is making some big errors. **Nonetheless, given that these are quite simple regression methods, we could consider that the results are reasonable**. More complex models could be used to achieve better predictions. However, these models would probably demand more time to build and more computing power to run, which could actually mean a worse cost-benefit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
